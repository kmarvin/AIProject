\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{Chung-et-al-TR2014}
\citation{Goodfellow-et-al-2016}
\citation{lstm_colah}
\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Recurrent neural networks}{1}{subsection.2.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rnn0}{{\caption@xref {fig:rnn0}{ on input line 196}}{1}{Recurrent neural networks}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An unrolled recurrent neural network. Source: \textit  {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}\relax }}{1}{figure.caption.1}}
\citation{hochreiter1997long}
\citation{Goodfellow-et-al-2016}
\citation{lstm_colah}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Long Short Term Memory Networks}{2}{subsection.2.2}}
\newlabel{fig:cell_state}{{\caption@xref {fig:cell_state}{ on input line 211}}{2}{Long Short Term Memory Networks}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Architecture of an LSTM module. Source: \textit  {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}\relax }}{2}{figure.caption.2}}
\newlabel{fig:forget}{{\caption@xref {fig:forget}{ on input line 220}}{2}{Long Short Term Memory Networks}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The forget gate. Source: \textit  {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}\relax }}{2}{figure.caption.3}}
\citation{gulli2017deep}
\newlabel{fig:input}{{\caption@xref {fig:input}{ on input line 229}}{3}{Long Short Term Memory Networks}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The input gate. Source: \textit  {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}\relax }}{3}{figure.caption.4}}
\newlabel{fig:output}{{\caption@xref {fig:output}{ on input line 244}}{3}{Long Short Term Memory Networks}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The output gate. Source: \textit  {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}\relax }}{3}{figure.caption.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Gated recurrent unit}{3}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Requirements}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Data Preprocessing}{4}{subsection.3.2}}
\newlabel{fig:onehot}{{\caption@xref {fig:onehot}{ on input line 280}}{4}{Data Preprocessing}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Example for an one hot encoding. Left: One hot encoding of the letter \textit  {a}. Right: One hot encoding of the word \textit  {and}. The encoding dimensions are \textit  {sequence length} $\times $ \textit  {number of classes}.\relax }}{4}{figure.caption.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Network Architecture}{4}{subsection.3.3}}
\newlabel{ssec:na}{{3.3}{4}{Network Architecture}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Parameters}{5}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Visualization}{5}{subsection.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{6}{section.4}}
\newlabel{sec:res}{{4}{6}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training and testing performance}{6}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Training of the network was performed using a two-layer GRU, a learning rate of $0.0004$, a batch size of $512$ and a hidden size of $256$. Input sequence length was 100 characters. The left image shows the training loss while the right figure displays the loss during testing.\relax }}{6}{figure.caption.7}}
\newlabel{fig:loss1}{{7}{6}{Training of the network was performed using a two-layer GRU, a learning rate of $0.0004$, a batch size of $512$ and a hidden size of $256$. Input sequence length was 100 characters. The left image shows the training loss while the right figure displays the loss during testing.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Schematic of a neural network training with early stopping. The network is repeatedly trained as long as the error on the test set (test error) is on the decrease. When the test error starts to increase, the training is halted. Source \textit  {https://elitedatascience.com/overfitting-in-machine-learning}\relax }}{7}{figure.caption.8}}
\newlabel{fig:stop}{{8}{7}{Schematic of a neural network training with early stopping. The network is repeatedly trained as long as the error on the test set (test error) is on the decrease. When the test error starts to increase, the training is halted. Source \textit {https://elitedatascience.com/overfitting-in-machine-learning}\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Determination of the optimal hidden size}{7}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Prediction accuracy on the test set. Left: Training was performed using a two-layer GRU, a learning rate of $0.001$, a batch size of $512$ and a hidden size of $128$. A maximum accuracy of $49\%$ is reached. Right: Training was performed using a two-layer GRU, a learning rate of $0.001$, a batch size of $512$ but a hidden size of $256$. Here a maximum accuracy of $51 \%$ could be achieved after 3 epochs of training.\relax }}{7}{figure.caption.9}}
\newlabel{fig:sub1}{{9}{7}{Prediction accuracy on the test set. Left: Training was performed using a two-layer GRU, a learning rate of $0.001$, a batch size of $512$ and a hidden size of $128$. A maximum accuracy of $49\%$ is reached. Right: Training was performed using a two-layer GRU, a learning rate of $0.001$, a batch size of $512$ but a hidden size of $256$. Here a maximum accuracy of $51 \%$ could be achieved after 3 epochs of training.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Prediction performance}{8}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Prediction of the next character}{8}{subsubsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Prediction accuracy on the test set. Training was performed using a two-layer GRU, a learning rate of $0.0004$, a batch size of $512$ and a hidden size of $256$. A maximum accuracy of almost $50\%$ is reached.\relax }}{8}{figure.caption.10}}
\newlabel{fig:truenet}{{10}{8}{Prediction accuracy on the test set. Training was performed using a two-layer GRU, a learning rate of $0.0004$, a batch size of $512$ and a hidden size of $256$. A maximum accuracy of almost $50\%$ is reached.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Prediction of the word ending}{8}{subsubsection.4.3.2}}
\newlabel{ssec:we}{{4.3.2}{8}{Prediction of the word ending}{subsubsection.4.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Text generation}{9}{subsection.4.4}}
\newlabel{ssec:tg}{{4.4}{9}{Text generation}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Interactive prediction}{9}{subsection.4.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{10}{section.5}}
\bibstyle{abbrvdin}
\bibdata{bib}
\bibcite{Chung-et-al-TR2014}{1}
\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix}{11}{section.6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Word Prediction Test\relax }}{11}{table.caption.12}}
\bibcite{Goodfellow-et-al-2016}{2}
\bibcite{gulli2017deep}{3}
\bibcite{lstm_colah}{4}
\@writefile{toc}{\contentsline {section}{\numberline {7}Literature}{12}{section.7}}
