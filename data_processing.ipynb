{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- encoding: iso-8859-15 -*-\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "\n",
    "# Import other python files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration / parameters to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_config(config_path = \"config.txt\", args = dict()):\n",
    "    with open(config_path) as source:\n",
    "        for line in source:\n",
    "            line = line.strip()\n",
    "            argLong, valueLong = line.split('=')\n",
    "            arg = argLong.strip()\n",
    "            value = valueLong.strip()\n",
    "            if value == 'True':\n",
    "                value = True\n",
    "            elif value == 'False':\n",
    "                value = False\n",
    "            elif '.' in value:\n",
    "                value = float(value)\n",
    "            else:\n",
    "                value = int(value)\n",
    "            args[arg] = value\n",
    "    return edict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 81, 'seq_len': 30, 'offset': 4, 'cuda': False, 'hidden_size': 128, 'clip': 50, 'num_layers': 3, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "config_path = 'config.txt'\n",
    "args = {}\n",
    "args = set_config(config_path, args)\n",
    "print(args)\n",
    "#args.batch_size = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_text(textsource):\n",
    "    text = ''\n",
    "    with open(textsource, encoding=\"utf8\") as txtsource:\n",
    "        for line in txtsource:\n",
    "            line = line.strip().lower()\n",
    "            line = line.replace(',', '').replace('.', '')\n",
    "            line = line.replace('»', '').replace('«', '')\n",
    "            line = line.replace('\"', '')\n",
    "            line = line.replace(u'\\ufeff', '')\n",
    "            text += ' ' + line\n",
    "    text = text[:1000] #### nachher wieder rauslöschen!!!\n",
    "    return text\n",
    "# Chevrons müssen noch weg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_len, offset):\n",
    "    # Get all the unique characters appearing in the text \n",
    "    chars = sorted(list(set(text)))\n",
    "    char_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "    print(char_idx)\n",
    "    print(len(char_idx))\n",
    "    idx_char = dict((i, c) for i, c in enumerate(chars)) #### das brauchen wir später!!!\n",
    "    no_classes = len(chars) # the nr. of unique characters corresponds to the nr. of classes\n",
    "    \n",
    "    # Define training samples by splitting the text\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - seq_len, offset):\n",
    "        sentences.append(text[i: i + seq_len])\n",
    "        next_chars.append(text[i + seq_len])\n",
    "    print('nr training samples', len(sentences))\n",
    "    \n",
    "    # Generate features and labels using one-hot encoding\n",
    "    X = np.zeros((len(sentences), seq_len, len(chars)), dtype='f')\n",
    "    y = np.zeros((len(sentences)))\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, char in enumerate(sentence):\n",
    "            X[i, j, char_idx[char]] = 1\n",
    "        y[i] = char_idx[next_chars[i]]\n",
    "        \n",
    "    return X, y, char_idx, idx_char, no_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    ''' A text dataset class which implements the abstract class torch.utils.data.Dataset. '''\n",
    "    def __init__(self, text, seq_len, offset):\n",
    "        self.data, self.target, self.char_idx, self.idx_char, self.no_classes = prepare_data(text, seq_len, offset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ''' Get the data for one training sample (by index) '''\n",
    "        return self.data[index,:,:], self.target[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Get the number of training samples '''\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM_RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, no_classes):\n",
    "        super(LSTM_RNN, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = no_classes, hidden_size = args.hidden_size, num_layers = 3)\n",
    "        self.linear = nn.Linear(in_features = args.hidden_size, out_features = no_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        #self.linear.weight.data.normal_(0, 0.075**2)\n",
    "        #self.linear.bias.data.normal_(0, 0.075**2)\n",
    "        #for name, param in self.lstm.named_parameters():\n",
    "        #    if 'bias' in name:\n",
    "        #        nn.init.constant(param, 0.0)\n",
    "        #    elif 'weight' in name:\n",
    "        #        nn.init.xavier_normal(param) \n",
    "        #nn.init.xavier_uniform(self.lstm.weight_hh_l0)\n",
    "\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Linear):\n",
    "        #         m.weight.data.normal_(0, 0.075*0.075)\n",
    "        #         m.bias.data.normal_(0, 0.075*0.075)\n",
    "                \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(args.num_layers, args.batch_size, args.hidden_size))\n",
    "        c0 = Variable(torch.zeros(args.num_layers, args.batch_size, args.hidden_size))\n",
    "        self.hidden = (h0, c0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = x.type(torch.DoubleTensor)\n",
    "        #print(x)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden) # (h0, c0 are set to default values)\n",
    "        #print(lstm_out)\n",
    "        #print(\"LSTM_OUT:\")\n",
    "        #print(lstm_out)\n",
    "        #lstm_out = lstm_out.view(-1, lstm_out.size(2))\n",
    "        #print(\"----------------\")\n",
    "        #print(lstm_out)\n",
    "        linear_out = self.linear(lstm_out[-1])\n",
    "        #print(\"Linear_OUT:\")\n",
    "        #print(linear_out)\n",
    "        \n",
    "        return linear_out\n",
    "        #res = self.softmax(linear_out) # use only the output of the last layer of lstm\n",
    "        #return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training loop (one epoch)\n",
    "def train(model, epoch):\n",
    "    hiddenLayers = model.init_hidden()\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss() # use the cross-entropy loss\n",
    "    total_loss = 0.0 # compute total loss over one epoch\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #print(data)\n",
    "        data = data.view(data.size(1), data.size(0), data.size(2))\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #print(\"data:\")\n",
    "        #print(data)\n",
    "        output = model(data)\n",
    "        print(\"output:\")\n",
    "        print(output)\n",
    "        #print(\"target:\")\n",
    "        #print(target)\n",
    "        loss = criterion(output, target.type(torch.LongTensor)) # check how far away the output is from the original data\n",
    "        #print(\"loss:\")\n",
    "        #print(loss)\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data[0]\n",
    "        #print(total_loss)\n",
    "\n",
    "\n",
    "    relative_loss = total_loss/float(len(train_loader))\n",
    "    print('Relative loss over epoch %s: %s' %(epoch, relative_loss))\n",
    "    return relative_loss # return the relative loss for later analysis\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction loop for ONE testdata tensor\n",
    "def rnn_predict(model, testdata):\n",
    "    ''' Note: testdata have to be submitted as a tensor'''\n",
    "    testdata = torch.from_numpy(testdata)\n",
    "    print(\"testdata:\")\n",
    "    print(testdata)\n",
    "    model.eval()\n",
    "    testdata = testdata.view(testdata.size(0), -1)\n",
    "    if args.cuda:\n",
    "        testdata = testdata.cuda()\n",
    "    testdata = testdata.type(torch.FloatTensor)\n",
    "    testdata = Variable(testdata)\n",
    "    prediction = model(testdata)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Function that returns the largest factor of number that isn't the number itself '''\n",
    "def lfactor(num):\n",
    "    for i in range(num - 1, 0, -1): # go backwards from num - 1 to 1\n",
    "        if num % i == 0:            # if a number divides evenly\n",
    "            return i                # it's the largest factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marvins test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# die funktion brauchen wir vllt gar nicht, je nachdem ob wir den test loader verwenden oder wie wir das auch immer machen\n",
    "def prepare_input(text):\n",
    "    X = np.zeros((1, args.seq_len, no_classes))  # array with one entry which have 20 lines, each 11 entrys\n",
    "    for t, char in enumerate(text):\n",
    "        X[0, t, char_idx[char]] = 1.\n",
    "    return X\n",
    "\n",
    "def sample(preds, top_n=1):\n",
    "    preds = preds.data.numpy()[0]\n",
    "    print(preds)\n",
    "    print(preds.shape)\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    return heapq.nlargest(top_n, range(len(preds)), preds.take)\n",
    "\n",
    "\n",
    "def predict_completion(model, text):\n",
    "    original_text = text\n",
    "    generated = text\n",
    "    completion = ''\n",
    "    next_char = ''\n",
    "    max_iterations = 30\n",
    "    i = 0\n",
    "    while next_char != ' ' and i < max_iterations:\n",
    "        i += 1\n",
    "        x = prepare_input(text)\n",
    "        preds = rnn_predict(model, x)\n",
    "        next_index = sample(preds, top_n=1)[0]\n",
    "        next_char = idx_char[next_index]\n",
    "        text = text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "    return completion\n",
    "\n",
    "\n",
    "def predict_completions(model, text, n=3):\n",
    "    x = prepare_input(text)\n",
    "    preds = model.rnn_predict(x, verbose=0)[0]\n",
    "    next_indices = sample(preds, n)\n",
    "    return [idx_char[idx] + predict_completion(text[1:] + idx_char[idx]) for idx in next_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': 4, 'h': 8, 'c': 3, 'e': 5, 'n': 14, 'z': 23, 'r': 17, ' ': 0, 'f': 6, 'i': 9, 'w': 22, 'a': 1, '–': 28, 'ö': 26, 'u': 20, 's': 18, 'g': 7, 'l': 12, 'ü': 27, 'o': 15, 't': 19, 'k': 11, 'j': 10, 'm': 13, 'p': 16, 'ß': 24, 'v': 21, 'b': 2, 'ä': 25}\n",
      "29\n",
      "nr training samples 243\n",
      "{'d': 4, 'h': 8, 'c': 3, 'e': 5, 'n': 14, 'z': 23, 'r': 17, ' ': 0, 'f': 6, 'i': 9, 'w': 22, 'a': 1, '–': 28, 'ö': 26, 'u': 20, 's': 18, 'g': 7, 'l': 12, 'ü': 27, 'o': 15, 't': 19, 'k': 11, 'j': 10, 'm': 13, 'p': 16, 'ß': 24, 'v': 21, 'b': 2, 'ä': 25}\n",
      "29\n",
      "nr training samples 243\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "# Generate train and test loader from our data\n",
    "train_text = prepare_text('./Brown_Leseprobe.txt')\n",
    "train_set = TextDataset(train_text, args.seq_len, args.offset)\n",
    "#args.batch_size = lfactor(len(train_set))\n",
    "train_loader = DataLoader(train_set, batch_size = args.batch_size, shuffle=True)\n",
    "\n",
    "test_text = prepare_text('./Brown_Leseprobe_test.txt')\n",
    "test_set = TextDataset(test_text, args.seq_len, args.offset)\n",
    "test_loader = DataLoader(test_set, batch_size = args.batch_size, shuffle=True)\n",
    "\n",
    "# set further parameters\n",
    "char_idx = train_set.char_idx\n",
    "idx_char = train_set.idx_char\n",
    "no_classes = train_set.no_classes\n",
    "input_shape = (args.seq_len, no_classes) # seq_len * nr. of unique characters \n",
    "\n",
    "# get len of data to determine the possible batch_size\n",
    "print(args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 29)\n",
      "LSTM_RNN(\n",
      "  (lstm): LSTM(29, 128, num_layers=3)\n",
      "  (linear): Linear(in_features=128, out_features=29)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Generate model\n",
    "print(input_shape)\n",
    "rnn = LSTM_RNN(no_classes)\n",
    "if args.cuda:\n",
    "    rnn.cuda()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the optimization algorithm\n",
    "optimizer = optim.RMSprop(rnn.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "output:\n",
      "Variable containing:\n",
      " 1.4951  0.7028 -1.7125  ...  -1.6399 -0.7114 -3.1551\n",
      " 1.4944  0.7021 -1.7118  ...  -1.6387 -0.7111 -3.1530\n",
      " 1.4944  0.7019 -1.7117  ...  -1.6387 -0.7110 -3.1528\n",
      "          ...             ⋱             ...          \n",
      " 1.4943  0.7019 -1.7112  ...  -1.6379 -0.7110 -3.1527\n",
      " 1.4946  0.7020 -1.7119  ...  -1.6390 -0.7110 -3.1534\n",
      " 1.4942  0.7021 -1.7115  ...  -1.6382 -0.7111 -3.1528\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.3148  0.6783 -1.5518  ...  -1.5019 -0.6893 -3.0700\n",
      " 1.3148  0.6786 -1.5516  ...  -1.5021 -0.6894 -3.0705\n",
      " 1.3155  0.6788 -1.5522  ...  -1.5031 -0.6895 -3.0720\n",
      "          ...             ⋱             ...          \n",
      " 1.3149  0.6787 -1.5520  ...  -1.5022 -0.6894 -3.0705\n",
      " 1.3145  0.6783 -1.5514  ...  -1.5019 -0.6892 -3.0695\n",
      " 1.3154  0.6790 -1.5525  ...  -1.5037 -0.6895 -3.0720\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.4306  0.7096 -1.6503  ...  -1.6006 -0.6255 -3.1880\n",
      " 1.4311  0.7102 -1.6511  ...  -1.6012 -0.6256 -3.1896\n",
      " 1.4311  0.7109 -1.6515  ...  -1.6020 -0.6259 -3.1904\n",
      "          ...             ⋱             ...          \n",
      " 1.4310  0.7100 -1.6509  ...  -1.6006 -0.6256 -3.1891\n",
      " 1.4300  0.7095 -1.6496  ...  -1.5994 -0.6255 -3.1869\n",
      " 1.4321  0.7107 -1.6523  ...  -1.6027 -0.6258 -3.1921\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 0: 2.7939666906992593\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6894  0.6593 -1.7604  ...  -1.6986 -0.7530 -3.2820\n",
      " 1.6894  0.6592 -1.7602  ...  -1.6984 -0.7531 -3.2817\n",
      " 1.6890  0.6590 -1.7598  ...  -1.6975 -0.7530 -3.2807\n",
      "          ...             ⋱             ...          \n",
      " 1.6898  0.6596 -1.7610  ...  -1.6997 -0.7533 -3.2828\n",
      " 1.6886  0.6586 -1.7592  ...  -1.6977 -0.7528 -3.2802\n",
      " 1.6888  0.6586 -1.7594  ...  -1.6973 -0.7529 -3.2802\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6545  0.7638 -1.8113  ...  -1.7523 -0.7467 -3.3017\n",
      " 1.6537  0.7629 -1.8100  ...  -1.7506 -0.7462 -3.2997\n",
      " 1.6535  0.7630 -1.8100  ...  -1.7504 -0.7464 -3.2993\n",
      "          ...             ⋱             ...          \n",
      " 1.6535  0.7628 -1.8099  ...  -1.7506 -0.7463 -3.2989\n",
      " 1.6548  0.7638 -1.8116  ...  -1.7526 -0.7467 -3.3019\n",
      " 1.6532  0.7628 -1.8096  ...  -1.7500 -0.7463 -3.2983\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6905  0.6260 -1.8364  ...  -1.6101 -0.6230 -3.2513\n",
      " 1.6911  0.6265 -1.8371  ...  -1.6114 -0.6231 -3.2528\n",
      " 1.6909  0.6264 -1.8367  ...  -1.6109 -0.6231 -3.2524\n",
      "          ...             ⋱             ...          \n",
      " 1.6906  0.6262 -1.8365  ...  -1.6105 -0.6232 -3.2519\n",
      " 1.6904  0.6261 -1.8361  ...  -1.6102 -0.6230 -3.2514\n",
      " 1.6899  0.6258 -1.8358  ...  -1.6093 -0.6230 -3.2501\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 1: 2.791005293528239\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6467  0.6456 -1.7626  ...  -1.7271 -0.7575 -3.3582\n",
      " 1.6451  0.6450 -1.7611  ...  -1.7258 -0.7572 -3.3551\n",
      " 1.6458  0.6453 -1.7620  ...  -1.7265 -0.7574 -3.3566\n",
      "          ...             ⋱             ...          \n",
      " 1.6459  0.6454 -1.7622  ...  -1.7268 -0.7574 -3.3568\n",
      " 1.6467  0.6454 -1.7626  ...  -1.7274 -0.7574 -3.3581\n",
      " 1.6449  0.6450 -1.7607  ...  -1.7252 -0.7571 -3.3546\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.5714  0.6218 -1.7630  ...  -1.5866 -0.5423 -3.2953\n",
      " 1.5717  0.6221 -1.7634  ...  -1.5879 -0.5424 -3.2964\n",
      " 1.5712  0.6217 -1.7629  ...  -1.5865 -0.5424 -3.2951\n",
      "          ...             ⋱             ...          \n",
      " 1.5725  0.6225 -1.7640  ...  -1.5884 -0.5425 -3.2981\n",
      " 1.5715  0.6218 -1.7629  ...  -1.5872 -0.5423 -3.2957\n",
      " 1.5719  0.6223 -1.7637  ...  -1.5874 -0.5424 -3.2964\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6100  0.5984 -1.8524  ...  -1.6826 -0.6670 -3.3549\n",
      " 1.6095  0.5978 -1.8518  ...  -1.6813 -0.6667 -3.3535\n",
      " 1.6101  0.5984 -1.8526  ...  -1.6825 -0.6671 -3.3551\n",
      "          ...             ⋱             ...          \n",
      " 1.6096  0.5983 -1.8521  ...  -1.6815 -0.6668 -3.3542\n",
      " 1.6108  0.5984 -1.8529  ...  -1.6829 -0.6672 -3.3565\n",
      " 1.6097  0.5980 -1.8519  ...  -1.6818 -0.6668 -3.3543\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 2: 2.7952173550923667\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6613  0.6579 -1.7676  ...  -1.7486 -0.7758 -3.4325\n",
      " 1.6611  0.6579 -1.7671  ...  -1.7483 -0.7758 -3.4324\n",
      " 1.6611  0.6578 -1.7671  ...  -1.7478 -0.7757 -3.4321\n",
      "          ...             ⋱             ...          \n",
      " 1.6611  0.6579 -1.7671  ...  -1.7481 -0.7757 -3.4322\n",
      " 1.6606  0.6574 -1.7667  ...  -1.7477 -0.7757 -3.4309\n",
      " 1.6612  0.6581 -1.7675  ...  -1.7483 -0.7758 -3.4323\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.4808  0.6964 -1.6245  ...  -1.6207 -0.7618 -3.3936\n",
      " 1.4799  0.6956 -1.6237  ...  -1.6191 -0.7616 -3.3908\n",
      " 1.4801  0.6960 -1.6236  ...  -1.6192 -0.7617 -3.3915\n",
      "          ...             ⋱             ...          \n",
      " 1.4812  0.6969 -1.6250  ...  -1.6215 -0.7620 -3.3945\n",
      " 1.4801  0.6956 -1.6235  ...  -1.6194 -0.7615 -3.3913\n",
      " 1.4806  0.6964 -1.6244  ...  -1.6200 -0.7619 -3.3928\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.5695  0.5948 -1.6851  ...  -1.6834 -0.7537 -3.4162\n",
      " 1.5684  0.5946 -1.6844  ...  -1.6818 -0.7535 -3.4136\n",
      " 1.5687  0.5945 -1.6844  ...  -1.6818 -0.7535 -3.4138\n",
      "          ...             ⋱             ...          \n",
      " 1.5692  0.5945 -1.6849  ...  -1.6825 -0.7534 -3.4152\n",
      " 1.5690  0.5948 -1.6846  ...  -1.6825 -0.7536 -3.4151\n",
      " 1.5688  0.5944 -1.6844  ...  -1.6825 -0.7534 -3.4145\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 3: 2.7946883042653403\n",
      "output:\n",
      "Variable containing:\n",
      " 1.7169  0.6595 -1.7895  ...  -1.7688 -0.7659 -3.4915\n",
      " 1.7176  0.6602 -1.7905  ...  -1.7702 -0.7662 -3.4937\n",
      " 1.7172  0.6595 -1.7898  ...  -1.7694 -0.7660 -3.4921\n",
      "          ...             ⋱             ...          \n",
      " 1.7164  0.6591 -1.7886  ...  -1.7681 -0.7657 -3.4904\n",
      " 1.7164  0.6591 -1.7889  ...  -1.7677 -0.7657 -3.4901\n",
      " 1.7167  0.6592 -1.7892  ...  -1.7688 -0.7658 -3.4910\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6224  0.6047 -1.6623  ...  -1.6444 -0.7529 -3.4642\n",
      " 1.6233  0.6051 -1.6633  ...  -1.6457 -0.7531 -3.4660\n",
      " 1.6229  0.6051 -1.6627  ...  -1.6451 -0.7532 -3.4648\n",
      "          ...             ⋱             ...          \n",
      " 1.6239  0.6055 -1.6636  ...  -1.6469 -0.7533 -3.4679\n",
      " 1.6234  0.6051 -1.6634  ...  -1.6462 -0.7532 -3.4666\n",
      " 1.6222  0.6048 -1.6622  ...  -1.6443 -0.7529 -3.4634\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.5932  0.6084 -1.7091  ...  -1.6894 -0.6513 -3.4611\n",
      " 1.5944  0.6088 -1.7102  ...  -1.6909 -0.6516 -3.4633\n",
      " 1.5928  0.6080 -1.7088  ...  -1.6888 -0.6511 -3.4597\n",
      "          ...             ⋱             ...          \n",
      " 1.5942  0.6088 -1.7104  ...  -1.6909 -0.6514 -3.4633\n",
      " 1.5936  0.6084 -1.7097  ...  -1.6895 -0.6513 -3.4614\n",
      " 1.5933  0.6085 -1.7095  ...  -1.6900 -0.6514 -3.4610\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 4: 2.784944534301758\n",
      "output:\n",
      "Variable containing:\n",
      " 1.7069  0.6782 -1.8030  ...  -1.7907 -0.7666 -3.5536\n",
      " 1.7080  0.6787 -1.8041  ...  -1.7920 -0.7668 -3.5556\n",
      " 1.7069  0.6780 -1.8028  ...  -1.7907 -0.7665 -3.5532\n",
      "          ...             ⋱             ...          \n",
      " 1.7076  0.6784 -1.8037  ...  -1.7914 -0.7666 -3.5552\n",
      " 1.7070  0.6781 -1.8032  ...  -1.7904 -0.7665 -3.5536\n",
      " 1.7071  0.6783 -1.8030  ...  -1.7903 -0.7665 -3.5539\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.5563  0.6213 -1.8344  ...  -1.8197 -0.7571 -3.5441\n",
      " 1.5571  0.6218 -1.8355  ...  -1.8214 -0.7573 -3.5463\n",
      " 1.5563  0.6213 -1.8344  ...  -1.8199 -0.7569 -3.5441\n",
      "          ...             ⋱             ...          \n",
      " 1.5558  0.6211 -1.8336  ...  -1.8192 -0.7569 -3.5425\n",
      " 1.5567  0.6212 -1.8351  ...  -1.8202 -0.7571 -3.5449\n",
      " 1.5561  0.6210 -1.8339  ...  -1.8196 -0.7570 -3.5436\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6408  0.6865 -1.7608  ...  -1.9031 -0.7619 -3.6093\n",
      " 1.6409  0.6865 -1.7610  ...  -1.9029 -0.7618 -3.6090\n",
      " 1.6413  0.6869 -1.7618  ...  -1.9036 -0.7621 -3.6106\n",
      "          ...             ⋱             ...          \n",
      " 1.6412  0.6867 -1.7613  ...  -1.9035 -0.7619 -3.6104\n",
      " 1.6414  0.6870 -1.7618  ...  -1.9036 -0.7621 -3.6107\n",
      " 1.6413  0.6872 -1.7620  ...  -1.9039 -0.7622 -3.6109\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 5: 2.7883199055989585\n",
      "output:\n",
      "Variable containing:\n",
      " 1.7058  0.6734 -1.7921  ...  -1.7734 -0.7501 -3.5851\n",
      " 1.7068  0.6736 -1.7932  ...  -1.7745 -0.7503 -3.5871\n",
      " 1.7065  0.6737 -1.7931  ...  -1.7738 -0.7503 -3.5869\n",
      "          ...             ⋱             ...          \n",
      " 1.7071  0.6737 -1.7934  ...  -1.7747 -0.7502 -3.5876\n",
      " 1.7062  0.6735 -1.7925  ...  -1.7734 -0.7502 -3.5860\n",
      " 1.7059  0.6733 -1.7924  ...  -1.7729 -0.7501 -3.5854\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.7068  0.5941 -1.8737  ...  -1.8748 -0.7665 -3.7018\n",
      " 1.7071  0.5945 -1.8740  ...  -1.8755 -0.7667 -3.7027\n",
      " 1.7075  0.5945 -1.8745  ...  -1.8760 -0.7667 -3.7038\n",
      "          ...             ⋱             ...          \n",
      " 1.7071  0.5944 -1.8741  ...  -1.8754 -0.7667 -3.7029\n",
      " 1.7076  0.5946 -1.8744  ...  -1.8756 -0.7668 -3.7037\n",
      " 1.7076  0.5947 -1.8747  ...  -1.8763 -0.7669 -3.7043\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.7131  0.5580 -1.9093  ...  -1.8782 -0.6496 -3.6526\n",
      " 1.7129  0.5579 -1.9092  ...  -1.8781 -0.6496 -3.6521\n",
      " 1.7134  0.5579 -1.9096  ...  -1.8788 -0.6497 -3.6531\n",
      "          ...             ⋱             ...          \n",
      " 1.7136  0.5583 -1.9101  ...  -1.8791 -0.6498 -3.6538\n",
      " 1.7136  0.5581 -1.9099  ...  -1.8793 -0.6497 -3.6538\n",
      " 1.7125  0.5578 -1.9089  ...  -1.8770 -0.6495 -3.6511\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 6: 2.792519489924113\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6351  0.6941 -1.7588  ...  -1.7402 -0.7307 -3.5737\n",
      " 1.6346  0.6938 -1.7581  ...  -1.7401 -0.7305 -3.5727\n",
      " 1.6345  0.6939 -1.7580  ...  -1.7395 -0.7305 -3.5726\n",
      "          ...             ⋱             ...          \n",
      " 1.6352  0.6944 -1.7592  ...  -1.7408 -0.7307 -3.5742\n",
      " 1.6351  0.6942 -1.7586  ...  -1.7403 -0.7307 -3.5737\n",
      " 1.6341  0.6938 -1.7575  ...  -1.7390 -0.7305 -3.5711\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6773  0.6334 -1.6825  ...  -1.7835 -0.6548 -3.6043\n",
      " 1.6786  0.6339 -1.6837  ...  -1.7855 -0.6550 -3.6073\n",
      " 1.6776  0.6336 -1.6826  ...  -1.7841 -0.6549 -3.6049\n",
      "          ...             ⋱             ...          \n",
      " 1.6779  0.6334 -1.6828  ...  -1.7845 -0.6549 -3.6056\n",
      " 1.6770  0.6333 -1.6818  ...  -1.7834 -0.6549 -3.6039\n",
      " 1.6784  0.6339 -1.6836  ...  -1.7852 -0.6551 -3.6069\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.7273  0.6134 -1.7737  ...  -1.8789 -0.7578 -3.7145\n",
      " 1.7267  0.6133 -1.7731  ...  -1.8779 -0.7578 -3.7133\n",
      " 1.7273  0.6135 -1.7738  ...  -1.8786 -0.7580 -3.7145\n",
      "          ...             ⋱             ...          \n",
      " 1.7268  0.6130 -1.7731  ...  -1.8779 -0.7577 -3.7131\n",
      " 1.7265  0.6130 -1.7727  ...  -1.8777 -0.7576 -3.7128\n",
      " 1.7268  0.6134 -1.7733  ...  -1.8780 -0.7578 -3.7136\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 7: 2.790623744328817\n",
      "output:\n",
      "Variable containing:\n",
      " 1.5899  0.6987 -1.7677  ...  -1.7454 -0.7334 -3.6274\n",
      " 1.5905  0.6993 -1.7686  ...  -1.7468 -0.7336 -3.6292\n",
      " 1.5897  0.6983 -1.7672  ...  -1.7454 -0.7334 -3.6268\n",
      "          ...             ⋱             ...          \n",
      " 1.5898  0.6985 -1.7675  ...  -1.7450 -0.7333 -3.6268\n",
      " 1.5898  0.6987 -1.7678  ...  -1.7456 -0.7335 -3.6274\n",
      " 1.5891  0.6982 -1.7667  ...  -1.7445 -0.7333 -3.6252\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6617  0.6999 -1.8448  ...  -1.8301 -0.6702 -3.7140\n",
      " 1.6609  0.6997 -1.8442  ...  -1.8290 -0.6702 -3.7120\n",
      " 1.6616  0.7001 -1.8449  ...  -1.8297 -0.6703 -3.7139\n",
      "          ...             ⋱             ...          \n",
      " 1.6613  0.6996 -1.8445  ...  -1.8295 -0.6701 -3.7131\n",
      " 1.6609  0.7000 -1.8445  ...  -1.8291 -0.6702 -3.7125\n",
      " 1.6605  0.6991 -1.8434  ...  -1.8283 -0.6700 -3.7112\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6055  0.6005 -1.7524  ...  -1.8711 -0.7494 -3.7187\n",
      " 1.6049  0.6003 -1.7519  ...  -1.8701 -0.7494 -3.7173\n",
      " 1.6063  0.6011 -1.7533  ...  -1.8726 -0.7497 -3.7210\n",
      "          ...             ⋱             ...          \n",
      " 1.6058  0.6007 -1.7527  ...  -1.8720 -0.7495 -3.7194\n",
      " 1.6057  0.6009 -1.7529  ...  -1.8714 -0.7495 -3.7194\n",
      " 1.6053  0.6004 -1.7523  ...  -1.8709 -0.7494 -3.7181\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 8: 2.7857465744018555\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6081  0.6900 -1.7760  ...  -1.7492 -0.7385 -3.6903\n",
      " 1.6058  0.6893 -1.7734  ...  -1.7469 -0.7381 -3.6859\n",
      " 1.6072  0.6899 -1.7749  ...  -1.7484 -0.7383 -3.6886\n",
      "          ...             ⋱             ...          \n",
      " 1.6068  0.6896 -1.7743  ...  -1.7477 -0.7381 -3.6880\n",
      " 1.6067  0.6899 -1.7745  ...  -1.7482 -0.7383 -3.6879\n",
      " 1.6058  0.6892 -1.7733  ...  -1.7460 -0.7381 -3.6847\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6535  0.7227 -1.6872  ...  -1.8013 -0.7334 -3.7230\n",
      " 1.6551  0.7233 -1.6888  ...  -1.8037 -0.7336 -3.7266\n",
      " 1.6542  0.7226 -1.6878  ...  -1.8019 -0.7334 -3.7242\n",
      "          ...             ⋱             ...          \n",
      " 1.6546  0.7229 -1.6883  ...  -1.8030 -0.7335 -3.7254\n",
      " 1.6550  0.7233 -1.6889  ...  -1.8035 -0.7336 -3.7264\n",
      " 1.6545  0.7232 -1.6883  ...  -1.8026 -0.7335 -3.7253\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6066  0.6742 -1.7487  ...  -1.8561 -0.7468 -3.7578\n",
      " 1.6068  0.6743 -1.7490  ...  -1.8560 -0.7468 -3.7581\n",
      " 1.6060  0.6737 -1.7482  ...  -1.8548 -0.7466 -3.7559\n",
      "          ...             ⋱             ...          \n",
      " 1.6068  0.6740 -1.7490  ...  -1.8565 -0.7468 -3.7580\n",
      " 1.6061  0.6738 -1.7480  ...  -1.8553 -0.7466 -3.7561\n",
      " 1.6062  0.6744 -1.7488  ...  -1.8559 -0.7469 -3.7570\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 9: 2.784471273422241\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6219  0.6704 -1.8002  ...  -1.7607 -0.7412 -3.7519\n",
      " 1.6227  0.6706 -1.8013  ...  -1.7615 -0.7414 -3.7536\n",
      " 1.6222  0.6705 -1.8006  ...  -1.7612 -0.7413 -3.7524\n",
      "          ...             ⋱             ...          \n",
      " 1.6217  0.6698 -1.7998  ...  -1.7602 -0.7411 -3.7510\n",
      " 1.6216  0.6702 -1.7999  ...  -1.7601 -0.7413 -3.7507\n",
      " 1.6223  0.6703 -1.8005  ...  -1.7611 -0.7412 -3.7527\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6473  0.7628 -1.8556  ...  -1.8203 -0.8278 -3.8008\n",
      " 1.6470  0.7623 -1.8551  ...  -1.8193 -0.8276 -3.7999\n",
      " 1.6471  0.7623 -1.8551  ...  -1.8194 -0.8276 -3.8000\n",
      "          ...             ⋱             ...          \n",
      " 1.6468  0.7620 -1.8546  ...  -1.8190 -0.8274 -3.7994\n",
      " 1.6472  0.7626 -1.8552  ...  -1.8199 -0.8277 -3.8001\n",
      " 1.6468  0.7621 -1.8546  ...  -1.8193 -0.8275 -3.7995\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.7157  0.7263 -1.8859  ...  -1.7018 -0.8122 -3.7686\n",
      " 1.7161  0.7266 -1.8865  ...  -1.7025 -0.8123 -3.7696\n",
      " 1.7170  0.7269 -1.8875  ...  -1.7032 -0.8125 -3.7714\n",
      "          ...             ⋱             ...          \n",
      " 1.7164  0.7266 -1.8867  ...  -1.7029 -0.8123 -3.7702\n",
      " 1.7163  0.7267 -1.8867  ...  -1.7027 -0.8124 -3.7702\n",
      " 1.7161  0.7268 -1.8866  ...  -1.7024 -0.8123 -3.7698\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 10: 2.7895313103993735\n",
      "output:\n",
      "Variable containing:\n",
      " 1.5993  0.6361 -1.8004  ...  -1.7704 -0.7276 -3.8000\n",
      " 1.5998  0.6368 -1.8011  ...  -1.7711 -0.7277 -3.8016\n",
      " 1.5995  0.6365 -1.8008  ...  -1.7709 -0.7276 -3.8008\n",
      "          ...             ⋱             ...          \n",
      " 1.5999  0.6365 -1.8012  ...  -1.7717 -0.7277 -3.8020\n",
      " 1.6003  0.6366 -1.8012  ...  -1.7716 -0.7278 -3.8024\n",
      " 1.5993  0.6362 -1.8003  ...  -1.7706 -0.7276 -3.8000\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.7201  0.5614 -1.8792  ...  -1.8363 -0.8218 -3.8563\n",
      " 1.7193  0.5607 -1.8780  ...  -1.8346 -0.8214 -3.8541\n",
      " 1.7199  0.5611 -1.8788  ...  -1.8356 -0.8216 -3.8557\n",
      "          ...             ⋱             ...          \n",
      " 1.7194  0.5609 -1.8782  ...  -1.8347 -0.8215 -3.8544\n",
      " 1.7191  0.5613 -1.8781  ...  -1.8346 -0.8216 -3.8543\n",
      " 1.7189  0.5610 -1.8776  ...  -1.8341 -0.8214 -3.8531\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.5816  0.7778 -1.9031  ...  -1.8749 -0.7256 -3.8420\n",
      " 1.5821  0.7783 -1.9042  ...  -1.8758 -0.7258 -3.8434\n",
      " 1.5824  0.7782 -1.9042  ...  -1.8758 -0.7259 -3.8438\n",
      "          ...             ⋱             ...          \n",
      " 1.5822  0.7786 -1.9043  ...  -1.8757 -0.7259 -3.8439\n",
      " 1.5818  0.7781 -1.9039  ...  -1.8748 -0.7258 -3.8425\n",
      " 1.5825  0.7786 -1.9047  ...  -1.8764 -0.7261 -3.8444\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 11: 2.7876757780710855\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6006  0.6291 -1.7886  ...  -1.7643 -0.7183 -3.8421\n",
      " 1.5998  0.6290 -1.7878  ...  -1.7628 -0.7181 -3.8403\n",
      " 1.5998  0.6292 -1.7881  ...  -1.7630 -0.7182 -3.8405\n",
      "          ...             ⋱             ...          \n",
      " 1.5989  0.6285 -1.7870  ...  -1.7617 -0.7179 -3.8377\n",
      " 1.6004  0.6291 -1.7886  ...  -1.7637 -0.7183 -3.8414\n",
      " 1.5990  0.6286 -1.7869  ...  -1.7622 -0.7179 -3.8383\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.5606  0.6329 -1.8294  ...  -1.8038 -0.7988 -3.8471\n",
      " 1.5603  0.6319 -1.8287  ...  -1.8034 -0.7986 -3.8458\n",
      " 1.5607  0.6324 -1.8290  ...  -1.8037 -0.7986 -3.8466\n",
      "          ...             ⋱             ...          \n",
      " 1.5604  0.6321 -1.8287  ...  -1.8030 -0.7986 -3.8455\n",
      " 1.5611  0.6326 -1.8298  ...  -1.8044 -0.7989 -3.8479\n",
      " 1.5604  0.6326 -1.8290  ...  -1.8032 -0.7987 -3.8463\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6014  0.6443 -1.8967  ...  -1.8785 -0.6481 -3.9179\n",
      " 1.6009  0.6438 -1.8959  ...  -1.8775 -0.6479 -3.9164\n",
      " 1.6025  0.6443 -1.8976  ...  -1.8798 -0.6482 -3.9201\n",
      "          ...             ⋱             ...          \n",
      " 1.6017  0.6442 -1.8968  ...  -1.8782 -0.6481 -3.9181\n",
      " 1.6017  0.6442 -1.8967  ...  -1.8788 -0.6481 -3.9181\n",
      " 1.6018  0.6443 -1.8973  ...  -1.8789 -0.6482 -3.9185\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 12: 2.7844940026601157\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6456  0.6397 -1.7952  ...  -1.7699 -0.7278 -3.8994\n",
      " 1.6471  0.6408 -1.7973  ...  -1.7724 -0.7282 -3.9036\n",
      " 1.6472  0.6405 -1.7971  ...  -1.7726 -0.7282 -3.9035\n",
      "          ...             ⋱             ...          \n",
      " 1.6459  0.6396 -1.7953  ...  -1.7705 -0.7277 -3.9001\n",
      " 1.6457  0.6396 -1.7952  ...  -1.7696 -0.7278 -3.8988\n",
      " 1.6454  0.6395 -1.7949  ...  -1.7700 -0.7276 -3.8989\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.5781  0.6334 -1.8191  ...  -1.7910 -0.7208 -3.8610\n",
      " 1.5791  0.6335 -1.8200  ...  -1.7920 -0.7209 -3.8627\n",
      " 1.5775  0.6328 -1.8183  ...  -1.7895 -0.7206 -3.8588\n",
      "          ...             ⋱             ...          \n",
      " 1.5788  0.6333 -1.8199  ...  -1.7913 -0.7209 -3.8620\n",
      " 1.5783  0.6336 -1.8198  ...  -1.7914 -0.7209 -3.8617\n",
      " 1.5788  0.6334 -1.8198  ...  -1.7913 -0.7209 -3.8621\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6146  0.7005 -1.7275  ...  -1.6970 -0.6469 -3.8589\n",
      " 1.6136  0.6998 -1.7265  ...  -1.6950 -0.6468 -3.8559\n",
      " 1.6147  0.7011 -1.7279  ...  -1.6980 -0.6472 -3.8599\n",
      "          ...             ⋱             ...          \n",
      " 1.6137  0.7000 -1.7265  ...  -1.6959 -0.6467 -3.8566\n",
      " 1.6139  0.7003 -1.7269  ...  -1.6961 -0.6469 -3.8574\n",
      " 1.6143  0.7002 -1.7270  ...  -1.6961 -0.6469 -3.8577\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 13: 2.7866713205973306\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6603  0.6381 -1.8110  ...  -1.7910 -0.7391 -3.9597\n",
      " 1.6599  0.6378 -1.8106  ...  -1.7900 -0.7390 -3.9583\n",
      " 1.6607  0.6386 -1.8119  ...  -1.7918 -0.7393 -3.9611\n",
      "          ...             ⋱             ...          \n",
      " 1.6611  0.6383 -1.8118  ...  -1.7914 -0.7393 -3.9612\n",
      " 1.6604  0.6379 -1.8110  ...  -1.7905 -0.7391 -3.9595\n",
      " 1.6605  0.6383 -1.8113  ...  -1.7910 -0.7392 -3.9598\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.6243  0.7187 -1.8544  ...  -1.8365 -0.6573 -3.9695\n",
      " 1.6237  0.7187 -1.8535  ...  -1.8359 -0.6572 -3.9688\n",
      " 1.6230  0.7181 -1.8525  ...  -1.8346 -0.6571 -3.9662\n",
      "          ...             ⋱             ...          \n",
      " 1.6231  0.7179 -1.8525  ...  -1.8345 -0.6570 -3.9658\n",
      " 1.6229  0.7182 -1.8525  ...  -1.8349 -0.6571 -3.9663\n",
      " 1.6239  0.7188 -1.8540  ...  -1.8363 -0.6572 -3.9690\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "output:\n",
      "Variable containing:\n",
      " 1.7090  0.6331 -1.7757  ...  -1.8818 -0.6747 -4.0071\n",
      " 1.7088  0.6332 -1.7755  ...  -1.8808 -0.6748 -4.0064\n",
      " 1.7090  0.6332 -1.7753  ...  -1.8814 -0.6747 -4.0066\n",
      "          ...             ⋱             ...          \n",
      " 1.7088  0.6333 -1.7753  ...  -1.8811 -0.6747 -4.0064\n",
      " 1.7087  0.6329 -1.7750  ...  -1.8802 -0.6746 -4.0051\n",
      " 1.7089  0.6331 -1.7752  ...  -1.8810 -0.6747 -4.0060\n",
      "[torch.FloatTensor of size 81x29]\n",
      "\n",
      "Relative loss over epoch 14: 2.7829501628875732\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run training and store history\n",
    "history = dict()\n",
    "history['loss_train'] = []\n",
    "history['loss_test'] = []\n",
    "\n",
    "# wie wir die accuracy machen, weiß ich noch nicht...\n",
    "#history['acc_train'] = []\n",
    "#history['acc_test'] = []\n",
    "print(args.batch_size)\n",
    "for epoch in range(15):\n",
    "    loss_train = train(rnn, epoch)        \n",
    "    history['loss_train'].append(loss_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAGHCAYAAAD2qfsmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XmcXFWd9/HPr7NAQkggxEBAAgQIhACBDoss6ZIBMaLg\noIwYQRGVGVRGBZ9RUVQGB2FGxR2XcUZFxqiPPo4KgsCA7BDoNoxAAoYdhGBYYiAQspznj1NNupss\n3dVVfWv5vF+v++quu1R+l4Sub59z7jmRUkKSJKkW2oouQJIkNS+DhiRJqhmDhiRJqhmDhiRJqhmD\nhiRJqhmDhiRJqhmDhiRJqhmDhiRJqhmDhiRJqhmDhiRJqhmDhqRBi4iTImJNRLQXXYuk+mLQkFQt\nLpwk6RUMGpIkqWYMGpKGRES8KiL+IyKeiIgXImJ+RLxrHee9PSJuj4i/RsTSiPjfiPhQj+PDI+Kz\nEXFv+X2WRMT1EXH40N6RpP4YXnQBkppfRGwKXAtMAb4OPAj8HfCDiBiXUvp6+bzXAT8GrgQ+Vr58\nGnAw8LXy638GPgF8F7gNGAvsB7QD/zMEtyNpAAwakobCPwC7ASeklH4CEBHfBq4D/iUi/jOl9Dxw\nFLA0pfT6DbzXUcClKaX317poSYNn14mkofAG4InukAGQUlpNbqUYA5TKu58FNouIDQWNZ4HpEbFL\nrYqVVD0GDUlDYQfgT+vYvwCI8nGAC4F7gd9GxCPlMR19Q8dngC2Ae8vjN/4tIvaqVeGSBsegIalu\npJT+AuwDHAP8CngtcFlEfL/HOdcDOwMnA38E3gt0RcR7hrxgSRtl0JA0FB4Cdl3H/mk9jgOQUlqV\nUro0pXRaSmln4DvAuyJiSo9znk0p/TCldAKwPfC/wNk1q15SxQwakobCb4FtIuL47h0RMQz4R2AZ\n+YkUImL8Oq79Y/nrJus6J6W0HFjUfVxSffGpE0nVEsB7I+IN6zj2VfKTJz+IiP1Y+3jrQcCHy0+c\nAHyvHCSuBh4FdgROA/6QUlpQPufuiPg90Ak8DewPHMfax18l1ZFIyVmDJQ1ORJwE/OcGTtkeeAk4\nHziaPPfFPcCXUko/6vE+xwJ/Tx6nsQXwBLk15J9TSk+WzzmTPIZjKrkV4yHgIuCL5SdZJNURg4Yk\nSaqZwsdoRMSpEXFHearhpRFxU0TM3sD5x0bEFRHxZI/zjxzKmiVJUv8UHjSAR4CPk6cPnknum/1V\nRExbz/kdwBXkCYDagWuA30TEjCGoVZIkDUBddp1ExFPA/0kpfX+jJ+fz7wR+klL6l9pWJkmSBqKu\nnjqJiDbgbcBo4OZ+XhPA5uTR55IkqY7UQ9cJEbFnRCwDVpCnID42pbSwn5f/E7AZ8LON/BmjI6I9\nIkYPrlpJklrLYD5D66LrJCKGA5OBceTn4U8BOjYWNiLiHeRZA49JKV2zkXMPBm4EuoDn+hy+HPhd\nZdVLktRUXg/0fShjDHlc5CEppZsG8mZ1ETT6iogrgUUbWgY6It4OfA84LqV0eT/e8x3Af1WvSkmS\nWs4JKaUfD+SCuhqj0UMbG5hOOCLmkEPG8f0JGWUPAlx88cVMm7a+B1oax+mnn86Xv/zlosuoGu+n\nfjXTvYD3U8+a6V6gue5nwYIFnHjiiVD+LB2IwoNGRHweuAx4mDyo8wSgBBxZPn4esG1K6aTy63cA\nPwA+BNwWEVuX3+qFlNJfN/BHvQgwbdo02tvba3AnQ2vcuHFNcR/dvJ/61Uz3At5PPWume4Hmu5+y\nFwd6QT0MBp0I/BBYCFxFnkvjyJTS1eXj25CnL+52CjAM+Cbw5x7bV4aqYEmS1D+Ft2iklN63keMn\n93l9WG0rkiRJ1VIPLRqSJKlJGTQa1Jw5c4ouoaq8n/rVTPcC3k89a6Z7gea7n0rV5eOttRAR7UBn\nZ2fnegfnPPzwwyxZsmRoC2thEyZMYPLkyUWXIUnaiK6uLmbOnAkwM6XUNZBrCx+jUS8efvhhpk2b\nxvLly4supWWMHj2aBQsWGDYkqYkZNMqWLFnC8uXLm2aejXrX/Uz2kiVLDBqS1MQMGn00yzwbkiTV\nAweDSpKkmjFoSJKkmjFoSJKkmjFoSJKkmjFotIgf/OAHtLW18fDDDxddiiSphRg0WkREEBFFlyFJ\najEGDUmSVDMtFzTWrCm6AkmSWkfLBY377iu6gvpx4YUXsueee7Lpppuy3Xbbcdppp7F06dJe5yxa\ntIi3vvWtTJo0iVGjRrH99tszZ84cli1b9vI5V155JbNmzWLLLbdk8803Z/fdd+dTn/rUUN+OJKkO\ntdzMoF1dcPzxRVdRvLPPPptzzjmHI488kg984APcc889XHjhhdx+++3ceOONDBs2jJUrV3LkkUey\ncuVKPvShD7HNNtvw2GOPcckll/Dss8+y+eabc/fdd3P00Uezzz778LnPfY5NNtmERYsWcdNNNxV9\ni5KkOtByQaOzs+gKirdkyRLOP/98Zs+ezW9/+9uX9++222784z/+IxdffDEnnXQSd999Nw8++CC/\n+MUvOPbYY18+76yzznr5+yuvvJKVK1dy2WWXseWWWw7pfUiS6l/LBY2uLkgJBvsAxvLlsHBhdWpa\nn913h9Gjq/++V111FStXruQjH/lIr/2nnHIKn/zkJ7n00ks56aSTGDduHACXX345s2fPZtSoUa94\nry222AKAX/7yl5x88sk+2SJJ6qXlgsYzz+SAMNgFWhcuhJkzq1PT+nR2Qi3Wd3vooYcAmDp1aq/9\nI0aMYMqUKS8f33HHHfnoRz/KBRdcwMUXX8ysWbM45phjOPHEExk7diwAxx9/PP/xH//BKaecwic+\n8QkOP/xw3vKWt3DccccZOiRJrRc02trg2msHHzR237323TC7717b9++PL3zhC7z73e/mV7/6FVdc\ncQUf+tCHOP/887nlllvYdttt2XTTTbnuuuu45ppruPTSS7n88sv56U9/yuGHH84VV1xh2JCkFtdy\nQWOPPeC66+DUUwf3PqNH16a1YSjssMMOpJS455572HHHHV/ev3LlSh544AFe97rX9Tp/+vTpTJ8+\nnU9+8pPccsstHHzwwXz729/mnHPOefmcww47jMMOO4wvfvGLnHfeeZx11llcc801/M3f/M1Q3ZYk\nqQ613OOt7e25RSOloispzhFHHMHIkSP52te+1mv/9773Pf7617/ypje9CYBly5axevXqXudMnz6d\ntrY2VqxYAcAzzzzzivefMWMGKaWXz5Ekta6Wa9Fob4eLLsrzaeyyS9HVFGPChAmceeaZnHPOOcye\nPZtjjjmGhQsX8q1vfYsDDjiAE044AYCrr76a0047jb/7u79j6tSprFq1iosuuojhw4dz3HHHAXDO\nOedw3XXX8cY3vpEddtiBxYsX861vfYvJkydz6KGHFnmbkqQ60HJBY5991o7TaNWgAfDZz36WiRMn\n8o1vfIMzzjiD8ePHc+qpp3LuuecybNgwILdMzJ49m0suuYTHHnuM0aNHM2PGDC6//HL2339/AN78\n5jfz0EMP8f3vf58lS5YwYcIEXvva13L22Wez+eabF3mLkqQ6EKlF+hAioh3o7Ozs5JRT2pk+Pbds\ndOvq6mLmzJl0dnbS3qiDLxqI/70lqXF0/8wGZqaUugZybcuN0QAolfKAUEmSVFstGzQeeihvkiSp\ndloyaHSPUbz22mLrkCSp2bVk0NhqK9hrL4OGJEm11pJBA3L3iUFDkqTaaumgcd998NhjRVciSVLz\natmg0dGRv/r0iSRJtdOyQWPixLywmt0nkiTVTsvNDNpTR8crg8aCBQuKKabF+N9ZklpDSweNUgm+\n8x1YvDiv/zF69GhOPPHEostqGaNHj2bChAlFlyFJqqGWDxoA118Pxx03mQULFrBkyZJii2ohEyZM\nYPLkyUWXIUmqoZYOGttumxdWu/ZaOO44mDx5sh98kiRVUcsOBu3mfBqSJNVOyweNjg744x/hqaeK\nrkSSpObT8kGj5zgNSZJUXS0fNHbYIW9O3CVJUvW1fNAAx2lIklQrBg1y0Jg/H5YuLboSSZKai0GD\nPCB0zRq44YaiK5EkqbkYNICdd85zath9IklSdRk0gIjcfeKAUEmSqsugUVYqwe23w3PPFV2JJEnN\nw6BRVirB6tVw001FVyJJUvMwaJTtthtMnOg4DUmSqsmgURaRnz4xaEiSVD0GjR5KJZg3D5YvL7oS\nSZKag0Gjh1IJVq6EW28tuhJJkpqDQaOH6dNh/Hi7TyRJqhaDRg9tbTBrlkFDkqRqMWj0USrBLbfA\nihVFVyJJUuMzaPRRKsGLL+ZBoZIkaXAMGn3MmAHjxjkduSRJ1WDQ6GPYMDj0UMdpSJJUDYUHjYg4\nNSLuiIil5e2miJi9gfO3iYj/ioh7ImJ1RFxQ7Zo6OvJU5CtXVvudJUlqLYUHDeAR4ONAOzATuBr4\nVURMW8/5mwBPAp8D5teioFIJnn8eOjtr8e6SJLWOwoNGSunSlNLlKaX7UkqLUkpnAc8Br1nP+Q+l\nlE5PKV0M/LUWNbW3w2ab2X0iSdJgFR40eoqItoh4OzAauLmoOkaMgEMOcUCoJEmDVRdBIyL2jIhl\nwArgQuDYlNLCImsqleCGG/LS8ZIkqTLDiy6gbCEwAxgHHAdcFBEdtQgbp59+OuPGjeu1b86cOcyZ\nM6fXvo4O+NSnYP58mDmz2lVIklSf5s6dy9y5c3vtW7p0acXvFymlwdZUdRFxJbAopfT+jZx3DfCH\nlNIZ/XjPdqCzs7OT9vb2jdawYgVssQWcey6csdF3lySpeXV1dTEz/9Y9M6XUNZBr66LrZB3ayE+X\nFGaTTeCggxwQKknSYBQeNCLi8xExKyJ2KI/VOA8oAReXj58XET/sc82MiNgHGAO8qvx6fY/DVqxU\nguuvhzVrqv3OkiS1hsKDBjAR+CF5nMZV5Lk0jkwpXV0+vg2wfZ9r/gB0kufeeAfQBVxa7cJKJXjm\nGbjzzmq/syRJraHwwaAppfdt5PjJ69g3JAHpwANh5MjcfbL33kPxJ0qS1FzqoUWjbo0aBQcc4DgN\nSZIqZdDYiFIpT9xVhw/nSJJU9wwaG1EqwV/+AgsLnT5MkqTGZNDYiIMPhuHD7T6RJKkSBo2N2Gyz\nPDOoQUOSpIEzaPRDqZSDhuM0JEkaGINGP5RK8PjjsGhR0ZVIktRYDBr9cOih0NbmsvGSJA2UQaMf\nxo6Fffd1nIYkSQNl0Oinjg6DhiRJA2XQ6KdSCR5+GB58sOhKJElqHAaNfpo1CyJs1ZAkaSAMGv00\nfjzstZcDQiVJGgiDxgB0z6chSZL6x6AxAB0dcN998NhjRVciSVJjMGgMQEdH/mqrhiRJ/WPQGICJ\nE2HaNIOGJEn9ZdAYoFLJAaGSJPWXQWOASiVYuBAWLy66EkmS6p9BY4C6x2nYqiFJ0sYZNAZo221h\nl10cpyFJUn8YNCrgfBqSJPWPQaMCpRLceSc89VTRlUiSVN8MGhUolfLX668vtg5JkuqdQaMCkyfD\nDjvYfSJJ0sYYNCrkOA1JkjbOoFGhUgnmz4dnny26EkmS6pdBo0KlEqQEN9xQdCWSJNUvg0aFpkyB\n7bZz4i5JkjbEoFGhiDxLqOM0JElaP4PGIJRK0NkJy5YVXYkkSfXJoDEIpRKsXg033VR0JZIk1SeD\nxiDsthtsvbXdJ5IkrY9BYxC6x2k4IFSSpHUzaAxSRwfMmwfLlxddiSRJ9cegMUilEqxcCbfcUnQl\nkiTVH4PGIE2fDuPHO05DkqR1MWgMUlub82lIkrQ+Bo0qKJVy18mKFUVXIklSfTFoVEFHRw4Z8+YV\nXYkkSfXFoFEFM2bAuHF2n0iS1JdBowqGDYNDDzVoSJLUl0GjSkqlPBX5ypVFVyJJUv0waFRJqZQn\n7ersLLoSSZLqh0GjSvbdFzbbzO4TSZJ6MmhUyYgRcMghBg1JknoyaFRRqQQ33ACrVhVdiSRJ9cGg\nUUWlEixbBvPnF12JJEn1waBRRfvvD5tu6rLxkiR1M2hU0ciRcNBBjtOQJKmbQaPKSiW4/npYs6bo\nSiRJKp5Bo8pKJXjmGfjjH4uuRJKk4hk0quzAA3MXit0nkiQZNKpu1KgcNhwQKkmSQaMmOjpy0Eip\n6EokSSqWQaMGSiX4y19gwYKiK5EkqVgGjRo4+GAYPtxxGpIkGTRqYLPNYL/9DBqSJBUeNCLi1Ii4\nIyKWlrebImL2Rq55bUR0RsSLEXFvRJw0VPX2V6nkOA1JkgoPGsAjwMeBdmAmcDXwq4iYtq6TI2JH\n4BLgf4AZwFeB70XE64ai2P7q6IDHH4dFi4quRJKk4gwvuoCU0qV9dp0VEe8HXgOsazjl+4H7U0of\nK7++JyIOBU4HrqxdpQNz6KHQ1pa7T3bdtehqJEkqRj20aLwsItoi4u3AaODm9Zz2GuCqPvt+BxxU\ny9oGauxY2Hdfx2lIklpbXQSNiNgzIpYBK4ALgWNTSgvXc/o2wOI++xYDYyNikxqWOWClUg4ajtOQ\nJLWqwrtOyhaSx1uMA44DLoqIjg2EjYqdfvrpjBs3rte+OXPmMGfOnGr/UZRKcMEF8NBDsOOOVX97\nSZKqbu7cucydO7fXvqVLl1b8fpHq8NftiLgSWJRSev86jl0LdKaUzuix793Al1NKW27gPduBzs7O\nTtrb22tQ9Ss9/TRMmADf/z6cVHfPxUiS1D9dXV3MnDkTYGZKqWsg19ZF18k6tAHr6wa5GTi8z74j\nWf+YjsKMHw977eU4DUlS6yo8aETE5yNiVkTsUB6rcR5QAi4uHz8vIn7Y45JvA1Mi4l8jYreI+AC5\nu+WCoa9+47rHaUiS1IoKDxrAROCH5HEaV5Hn0jgypXR1+fg2wPbdJ6eUHgTeCBwBzCc/1vrelFLf\nJ1HqQqkE998Pjz5adCWSJA29wgeDppTet5HjJ69j33XkQFL3Ojry12uvhRNOKLYWSZKGWj20aDS1\nV70Kpk3L05FLktRqDBpDwHEakqRWVVHQiIiTIuKNPV7/W0Q8W14QbYfqldccSiW45x544omiK5Ek\naWhV2qLxSeAFgIg4CPgg8DFgCfDl6pTWPEql/NXuE0lSq6k0aGwPdK9L+rfAL1JK3wXOBGZVo7Bm\nMmlSXljN7hNJUqupNGg8B2xV/v5I1q6a+iIwarBFNaOODls0JEmtp9KgcSXwvYj4HjAV+G15/3Tg\nwSrU1XRKJbjzTliypOhKJEkaOpUGjQ+Sp/x+FfDWlNJT5f0zgbnrvaqFdY/TuP76YuuQJGkoVTRh\nV0rpWeC0dez/7KAralKTJ+cVXK+9Fo49tuhqJEkaGpU+3jo7Ig7t8fqDETE/In4cEetdQbXVOZ+G\nJKnVVNp18gVgLEBE7AV8iTxOYyfqdHGzetDRAXfcAc8+W3QlkiQNjUqDxk7A3eXv3wpcklL6JHns\nxhuqUVgzKpUgJbjhhqIrkSRpaFQaNF4CRpe/PwK4ovz905RbOvRKU6bAdtvZfSJJah2Vrt56A3BB\nRNwIHAAcX94/FXBB9PWIcJyGJKm1VNqicRqwCjgOeH9K6bHy/jcAl1ejsGZVKkFXFyxbVnQlkiTV\nXqWPtz4MvGkd+08fdEVNrlSC1avhppvg9a8vuhpJkmqr0q4TImIYeZ2TaeVddwG/TimtrkZhzWrq\nVNh669x9YtCQJDW7ioJGROxCfpx1O+Ce8u4zgUci4o0ppfuqVF/TiciPuTpOQ5LUCiodo/E14D5g\n+5RSe0qpHZgMPFA+pg0oleC222D58qIrkSSptioNGiXgYymlp7t3lNc7+UT5mDagVIKVK+Hmm4uu\nRJKk2qo0aKwANl/H/jHkOTa0AXvsAVtt5bLxkqTmV2nQuAT4bkQcGGu9Bvg28Ovqldec2tpg1izH\naUiSml+lQeND5DEaNwMvlrebgEXAR6pTWnMrleCWW+DFF4uuRJKk2hnMMvFvLj990v1464KU0qKq\nVdbkSiVYsQLmzctPoUiS1Iz6HTQiYmOrsh4WEQCklM4YTFGtYO+9Ydy43H1i0JAkNauBtGjs28/z\nUiWFtJphw/I4DQeESpKaWb+DRkrpsFoW0oo6OuDss/OjriNGFF2NJEnVV+lgUFVBqZQn7br99qIr\nkSSpNgwaBWpvhzFjfMxVktS8DBoFGj4cDjnEoCFJal4GjYKVSnDjjbBqVdGVSJJUfQaNgnV0wLJl\nMH9+0ZVIklR9Bo2C7b8/jBpl94kkqTkZNAo2ciQcdJBBQ5LUnAwadaBUguuvh9Wri65EkqTqMmjU\ngVIJnn0W7ryz6EokSaoug0YdOOCA3IVi94kkqdkYNOrAqFFw4IEGDUlS8zFo1IlSKS+wllySTpLU\nRAwadaJUgiVL4O67i65EkqTqMWjUiYMOylOSu2y8JKmZGDTqxGabwX77OU5DktRcDBp1pFTKQcNx\nGpKkZmHQqCOlEjzxBPzpT0VXIklSdRg06sghh0Bbm90nkqTmYdCoI2PHQnu7QUOS1DwMGnWmo8Nx\nGpKk5mHQqDOlEjz6KDz4YNGVSJI0eAaNOjNrFkTYfSJJag4GjTqz5Zaw994GDUlSczBo1KHu+TQk\nSWp0Bo061NEBDzwAjzxSdCWSJA2OQaMOdXTkr657IklqdAaNOvSqV8Eee9h9IklqfAaNOuU4DUlS\nMzBo1KlSCe69Fx5/vOhKJEmqnEGjTnWP07j++mLrkCRpMAwadWrSJNh1V7tPJEmNrfCgERFnRsS8\niPhrRCyOiF9GxNR+XPfBiLg7IpZHxIKIeOdQ1DuUHKchSWp0hQcNYBbwdeBA4AhgBHBFRIxa3wUR\n8X7gXOAzwB7A2cA3I+KNNa92CJVKcNddsGRJ0ZVIklSZ4UUXkFI6qufriHg38CQwE7hhPZedCHwn\npfTz8usHI2J/4OPApTUqdciVSvnrddfBW95SbC2SJFWiHlo0+toCSMDTGzhnE+DFPvteBA6IiGG1\nKmyobb897LijE3dJkhpXXQWNiAjgK8ANKaW7N3Dq74D3RUR7+br9gPeSu10m1LzQIeQ4DUlSIyu8\n66SPC8ljLg7ZyHmfA7YGbo6INuAJ4AfAx4A1G7rw9NNPZ9y4cb32zZkzhzlz5lRYcm2VSnDRRfDM\nM3llV0mSamnu3LnMnTu3176lS5dW/H6RUhpsTVUREd8AjgZmpZQe7uc1w8iB43HgH4DzU0pbrOfc\ndqCzs7OT9vb2KlVde/ffDzvvDL/+NRx9dNHVSJJaUVdXFzNnzgSYmVLqGsi1ddF1Ug4ZbwYO62/I\nAEgprU4p/TnltPR24De1qrEoO+0Er3613SeSpMZUeNdJRFwIzAGOAZ6PiK3Lh5amlF4sn/N5YLuU\n0knl17sCBwC3AuOBM4DpwLuGuPyai8izhDogVJLUiOqhReNUYCzwe+DPPba39ThnErB9j9fDgI8C\n88kDQ0cCBw+kNaSRlErQ1QXLlhVdiSRJA1N4i0ZKaaNhJ6V0cp/XC4HGGWgxSKUSrF4NN94Is2cX\nXY0kSf1XDy0a2oipU2HrrR2nIUlqPAaNBhDhfBqSpMZk0GgQHR1w222wfHnRlUiS1H8GjQZRKsGq\nVXDzzUVXIklS/xk0GsQee8BWW9l9IklqLAaNBtHWlrtPDBqSpEZi0GggpRLceiu82HfdWkmS6pRB\no4F0dMCKFTBvXtGVSJLUPwaNBrL33jBunN0nkqTGYdBoIMOGwaxZBg1JUuMwaDSYUgluugleeqno\nSiRJ2jiDRoMpleCFF+D224uuRJKkjTNoNJh994UxY1w2XpLUGAwaDWb4cDjkEMdpSJIag0GjAR12\nGFxzDXzlK47VkCTVN4NGA/rAB+Ckk+CjH4Xp0+GXv4SUiq5KkqRXMmg0oM03h+98B+64A3beGd7y\nFnjtax0gKkmqPwaNBrbnnnD55XDZZfDUU7D//vDOd8IjjxRdmSRJmUGjCcyeDfPn51aOK66AqVPh\nrLNg2bKiK5MktTqDRpMYPhz+/u/hT3+CM86AL30Jdt0V/v3fYfXqoquTJLUqg0aTGTsWzj0X7rkH\njjgih4999sktHZIkDTWDRpOaPBkuvjiv9LrFFvD618Mb3gB33VV0ZZKkVmLQaHL7759nEf3FL3K3\nyt57w6mnwuLFRVcmSWoFBo0WEJEfgb37bvjiF+GnP83jN847L6+bIklSrRg0WsjIkXD66bBoEbzn\nPfCZz8Duu8N//ResWVN0dZKkZmTQaEFbbZWnL7/rLmhvhxNPhNe8Bq6/vujKJEnNxqDRwqZOzdOX\n//73uUWjowPe+tbc4iFJUjUYNESplJ9Oueii/HWPPfJcHM88U3RlkqRGZ9AQAG1tefrye+7JYze+\n+928jspXv+oKsZKkyhk01Mvo0Xn68kWL4LjjcsvG9Onw3//tCrGSpIEzaGidttkmt2rMnw877QTH\nHptXiO3sLLoySVIjMWhog/baC373O/jtb2HJEthvP3jXu1whVpLUPwYNbVREnr78jjvg29/OwWPq\nVPj0p10hVpK0YQYN9dvw4fAP/5CnMj/9dPjCF1whVpK0YQYNDdjYsfD5z8O9965dIXbffV0hVpL0\nSgYNVax7hdhbb4Vx4/IKsUcd5QqxkqS1DBoatAMOyCvE/vznuZVj773h/e+HJ58sujJJUtEMGqqK\niDx9+V135bEbP/kJ7LKLK8RKUqszaKiqNtkkT/LVd4XYH//YFWIlqRUZNFQTfVeIPeEEOOgguPHG\noiuTJA0lg4ZqqucKsatWwaGH5qnN77uv6MokSUPBoKEhUSrBbbflFWJvuQWmTYOPftQVYiWp2Rk0\nNGS6V4i99948duM738kDRr/6VQeMSlKzGl50AWo93SvEvve9OXCccQZ85COw5ZZ5Mbe+26RJvV9v\ntVUOLZKk+mfQUGEmTcrTl59xBsybB088kbfHH4c//xm6uvLrpUt7XzdsGGy99bpDSN9tzJhi7k2S\nlBk0VLhp0/K2Pi+8AIsXrw0h3YGke/vjH+HKK/P3L73U+9oxY/rXSjJxYl7LRZJUXf5oVd0bNQp2\n3DFvG5JSHlzaN4j0bClZuDB/v2RJ72sjYMKEjbeSTJqUp1uPqNXdSlJzMWioaUTA+PF522OPDZ+7\ncmWeIr3op//FAAARNklEQVRvEOn+/r778pwfjz8Oy5f3vnaTTTbcSjJpUm6h2Xzz2t2rJDUKg4Za\n0ogRsN12eduY555bd5dN93bbbfn4k0/C6tVrr5s6NU9W1r3tu28OQZLUSgwa0kaMGQO77pq3DVm9\nGp56Ch59NI8b6erK229+A88/n8/Zccfe4aO9PQ9slaRmZdCQqmTYsDyodOLEHCBOOinvX706r/3S\n1QWdnfnrF76w9mmabbd9Zfh49asdByKpORg0pBobNgx22y1vc+bkfSnBAw+sbfXo6oILL1w7SHXC\nhFeGjylTDB+SGo9BQypARA4OU6bktV8gh4/HHusdPn70Izj//Hx83LhXho9dd81BRpLqlUFDqhMR\nucvk1a+GY45Zu3/xYvjDH9aGj//3/+BLX8rHNtsM9tmnd/iYNi0PdpWkemDQkOrc1lvD7Nl56/b0\n0zB//tpxH7/7HXz96/nYJpvA3nv3Dh977gmbblpM/ZJam0FDakDjx8Pf/E3euv31r3DHHWtbPm68\nMU/xvmZNnvV0+vTe4WPGjNwiIkm1ZNCQmsTYsTBrVt66LV/e+1Hbri64+OI8YVlbWx6gOnPm2vCx\nzz55LIgkVUvhQSMizgSOBXYHXgBuAj6eUrp3I9edAPwTsCuwFLgM+KeU0tO1rVhqHKNHw4EH5q3b\nSy/BXXf1Dh+/+EVeUwZgl116t3zstlsOJWvW5AGr3VvP15UeK+J9Jk2C/ffPwUxS7RUeNIBZwNeB\n28n1nAdcERHTUkovrOuCiDgE+CHwYeASYDvgO8B3geOGomipUY0cmWcp3XdfeO97875Vq+Cee9YG\nj85O+Jd/ybOiNqOIPGj2gAPWBrE993QQrVQLhQeNlNJRPV9HxLuBJ4GZwA3ruew1wAMppW+WXz8U\nEd8BPlarOqVm1j2GY/p0eOc78741a/JEY/fdlz+Yu7e2tnV/X+mxoTwvAh58EG69de128cU5aI0a\nlVtwDjxwbQDZYQfnLpEGq/CgsQ5bAAnYUBfIzcC5EfGGlNJlEbE18HfApUNRoNQK2tryei1TpxZd\nSXXtvnveumdufeGF/Phwd/D45S/hggvysYkTeweP/feHLbYornapEdVV0IiIAL4C3JBSunt956WU\nboqIE4GfRsSm5Pv4NXDa0FQqqVmMGgUHH5y3bk8+CfPm5e3WW/O8Jc8+m4/tttva7pYDD4S99srd\nUVI9Wr0aVqzovb344sD3Pfhg5TXUVdAALgT2AA7Z0EkRsQfwVeBs4ApgEvBF8jiN99W2REnNbuJE\neNOb8ga5G+lPf1obPG69FebOzU/vbLLJK7tcdtrJLpdWt3JlDqeD+XCvxr5Vqyqrv60t/9vu3gbz\n7zlSSpVfXUUR8Q3gaGBWSunhjZx7EbBpSultPfYdAlwPTEopLV7HNe1AZ0dHB+P6PL83Z84c5nQv\nQiFJ/fDii3nStO7gMW9eHs8Cea2angNNDzgAttyy2HpVfc88k//O779/7dfu7x95JAfUgRg5cu0H\n+6ab9v6g39D+au/77/+ey89+NrdXbUuXLuW6664DmJlS6hrIfdVF0CiHjDcDpZTS/f04/+fASyml\nd/TYdxB58Oh2KaUn1nFNO9DZ2dlJe3t79YqXpLIlS3p3ucybl2dxhbwuTc/gMWNG/qGu+rVqFTz6\n6PrDRHd3GuQgOWUK7Lzz2q8TJrzyg3x9H/YjR9Z3K1hXVxczZ86ECoJG4V0nEXEhMAc4Bni+PLAT\nYGlK6cXyOZ8nB4jy8C1+A3w3Ik4FfgdsC3wZuHVdIUOShsKECXDUUXmDPG/HokW9u1x+9rM8l0n3\nY8Y9Wz523rm+P2ya0bJlvUNEz68PPbS266GtDSZPziGivT0vhtgdKqZMscVqQwoPGsCp5KdMft9n\n/8nAReXvJwHbdx9IKf0wIsYAHySPzXgW+B/gE7UuVpL6KyK3ZOy6K5xwQt63YkWeKr47eFx++dp1\nasaP7x089t8/hxdVbs0a+POfX9ka0f11yZK1544ZszY8HHts7xaKyZMd9Fupuug6GQp2nUiqV08/\n3bvL5dZb4amn8rGdd+490HSffVwgr6/ly+GBB9bdxfHAAzncddtuu97dG90tEt1dHbYorVtDd51I\nUqsbP773Cr0p5Q/JnsHjF7/IH5gjRuTxHd0tHuPH59+0B7INH95YH6gpweLF62+VeKJHh/mmm64N\nDq9/fe8wsdNOhrQiGDQkqc5E5A/InXeG7gfiXnoJ/vd/1waPq66Cb35zw++zIT2Dx4gRAw8r/dkG\n+r4rV64NEn1bJpYvX1v71luvDROHH967dWKbbfJ4CtUPg4YkNYCRI2G//fL2wQ/mfc89B88/n0PI\nhraVKzd+Tn+2554b+J9TiREjcuvDlClQKsHJJ68NEzvtlMdSqHEYNCSpQY0ZU98fuinlpzb6G2Ta\n2nKQ2G47GDas6OpVLQYNSVJNROTWiREjYLPNiq5GRbEnS5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxB\nQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik\n1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBQ5Ik1YxBo0HNnTu36BKqyvupX810L+D91LNmuhdo\nvvuplEGjQTXbP2Dvp341072A91PPmuleoPnup1IGDUmSVDMGDUmSVDMGDUmSVDPDiy5gCG0KsGDB\ngqLrqIqlS5fS1dVVdBlV4/3Ur2a6F/B+6lkz3Qs01/30+OzcdKDXRkqputXUqYg4GLix6DokSWpg\nh6SUbhrIBa0UNEYDuxddhyRJDWxhSmn5QC5omaAhSZKGnoNBJUlSzRg0JElSzRg0JElSzRg0JElS\nzbRE0IiID0bEAxHxQkTcEhH7F11TJSJiVkT8OiIei4g1EXFM0TUNRkScGRHzIuKvEbE4In4ZEVOL\nrqsSEXFqRNwREUvL200RMbvouqolIj5R/jd3QdG1VCIiPluuv+d2d9F1VSoito2IH0XEkohYXv63\n1150XZUo/2zu+3ezJiK+XnRtlYiItoj4XETcX/67WRQRZxVdV6UiYkxEfCUiHizfzw0Rsd9A3qPp\ng0ZEHA98CfgssC9wB/C7iJhQaGGV2QyYD3wAaIbHhWYBXwcOBI4ARgBXRMSoQquqzCPAx4F2YCZw\nNfCriJhWaFVVUA7mf0/+f6eR3QlsDWxT3g4ttpzKRMQW5DmBVgCvB6YBHwWeKbKuQdiPtX8n2wCv\nI/98+1mRRQ3CJ4B/IP+c3h34GPCxiDit0Koq9x/A4cAJwJ7AlcBVETGpv2/Q9I+3RsQtwK0ppQ+X\nXwf5Q+FrKaV/K7S4QYiINcDfppR+XXQt1VIOf08CHSmlG4quZ7Ai4ing/6SUvl90LZWKiDFAJ/B+\n4NPAH1JKZxRb1cBFxGeBN6eUGvK3/p4i4nzgoJRSqehaaiEivgIclVJq1NbN3wBPpJRO6bHv58Dy\nlNK7iqts4CJiU2AZcHRK6fIe+28HfptS+kx/3qepWzQiYgT5t8v/6d6XcrK6CjioqLq0XluQf5N5\nuuhCBqPcdPp2YDRwc9H1DNI3gd+klK4uupAq2LXc7XhfRFwcEdsXXVCFjgZuj4iflbscuyLifUUX\nVQ3ln9knkH+LblQ3AYdHxK4AETEDOAT4baFVVWY4MIzcetbTCwygRbDZ1zqZQP6PtLjP/sXAbkNf\njtan3NL0FeCGlFJD9p1HxJ7kYNH9W8CxKaWFxVZVuXJY2ofctN3obgHeDdwDTALOBq6LiD1TSs8X\nWFclppBbmL4EnAscAHwtIlaklH5UaGWDdywwDvhh0YUMwvnAWGBhRKwm/0L/qZTST4ota+BSSs9F\nxM3ApyNiIfmz8x3kX9T/1N/3afagocZxIbAHOfk3qoXADPIPyuOAiyKioxHDRkS8mhz8jkgprSy6\nnsFKKf2ux8s7I2Ie8BDwNqDRurbagHkppU+XX99RDrmnAo0eNN4DXJZSeqLoQgbhePKH8duBu8lh\n/asR8ecGDYInAv8JPAasArqAH5N7C/ql2YPGEmA1eQBYT1sDjfwPualExDeAo4BZKaXHi66nUiml\nVcD95Zd/iIgDgA+Tf/tsNDOBVwFd5dYmyK2DHeVBbZukBh7glVJaGhH3ArsUXUsFHgf6LkO9AHhL\nAbVUTURMJg8K/9uiaxmkfwPOSyn93/LruyJiR+BMGjAIppQeAA4rD9Ifm1JaHBE/Ye3Puo1q6jEa\n5d/EOskjZoGXm+gPJ/ejqWDlkPFm4LCU0sNF11NlbcAmRRdRoauAvci/jc0ob7cDFwMzGjlkwMuD\nXHchf2g3mht5ZdfvbuQWmkb2HnLTfCOOZehpNPkX3J7W0OCftymlF8ohY0vy007/3d9rm71FA+AC\n4AcR0QnMA04n/0P4QZFFVSIiNiP/cOz+DXNKeaDR0ymlR4qrrDIRcSEwBzgGeD4iuluelqaUXiyu\nsoGLiM8DlwEPA5uTB7SVgCOLrKtS5XELvcbKRMTzwFMppb6/Tde9iPgC8Bvyh/F2wD8DK4G5RdZV\noS8DN0bEmeRHQA8E3gecssGr6lj5F8B3Az9IKa0puJzB+g1wVkQ8CtxFfuT9dOB7hVZVoYg4kvyZ\ncw+wK7nF5m4G8Bna9EEjpfSz8mOT55C7TOYDr08p/aXYyiqyH3AN+cmMRB4MBnng1HuKKmoQTiXf\nx+/77D8ZuGjIqxmcieS/h0nAUuB/gSOb5GmNbo3civFqcr/yVsBfgBuA16SUniq0qgqklG6PiGPJ\ngw4/DTwAfLgRBxv2cASwPY03XmZdTgM+R35iayLwZ+Bb5X2NaBxwHjmgPw38HDgrpdS31Wa9mn4e\nDUmSVJyG7jOSJEn1zaAhSZJqxqAhSZJqxqAhSZJqxqAhSZJqxqAhSZJqxqAhSZJqxqAhSZJqxqAh\nqWFFRCki1kTE2KJrkbRuBg1Jjc7pjaU6ZtCQJEk1Y9CQVLHIzoyI+yNieUT8ISLeWj7W3a1xVETc\nEREvRMTNETG9z3u8NSLujIgXI+KBiDijz/GREfGvEfFw+Zx7I+LkPqXsFxG3RcTzEXFjROxa41uX\n1E8GDUmD8UngRODvgT3IS5j/KCJm9Tjn38jLZO9HXjn11xExDCAiZgI/Ja+suifwWeBzEfGuHtf/\nCDievCrm7uQl0Z/rcTyAfyn/GTOBVcB/VvUuJVXM1VslVSQiRpKXjT48pXRrj/3/DowC/h24Bnhb\nSunn5WNbAo8CJ6WUfh4RFwMTUkqze1z/r8BRKaW9ImIqsLD8Z1yzjhpKwNXl478v73sDcAkwKqX0\nUg1uXdIA2KIhqVK7AKOBKyNiWfcGvBPYuXxOAm7pviCl9AxwDzCtvGsacGOf970R2DUiAphBbqG4\nbiO1/LHH94+Xv04c2O1IqoXhRRcgqWGNKX89Cvhzn2MryEFksF7o53kre3zf3UzrL1JSHfB/REmV\nupscKHZIKd3fZ3usfE4Ar+m+oNx1MrV8LcAC4JA+73socG/K/bp/JP+cKtXwPiTVkC0akiqSUnou\nIr4IfLk8uPMGYBw5OCwFHi6f+pmIeBp4EjiXPCD0V+VjXwLmRcRZ5EGhBwMfBE4t/xkPRcRFwH9G\nxIeBO4AdgIkppf9bfo9YR3nr2iepAAYNSRVLKX06Ip4EPgFMAZ4FuoDPA8PI3RifAL5K7kr5A3B0\nSmlV+fo/RMTbgHOAs8jjK85KKf2oxx9zavn9vglsRQ4wn+9ZxrpKq9Y9ShocnzqRVBM9ngjZMqX0\n16LrkVQMx2hIqiW7MKQWZ9CQVEs2mUotzq4TSZJUM7ZoSJKkmjFoSJKkmjFoSJKkmjFoSJKkmjFo\nSJKkmjFoSJKkmjFoSJKkmjFoSJKkmjFoSJKkmvn/LI63vCaBETMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f15149c4b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try a prediction\n",
    "\n",
    "#testdata = Variable(torch.from_numpy(test_set.data[0])) # get first element from the test set\n",
    "#truth = test_set.target[0]\n",
    "#print(testdata,truth)\n",
    "\n",
    "#prediction = rnn(testdata)\n",
    "## dann muss man hier noch auf die sizes achten, ach verdammt\n",
    "#prepare_input(\"This is an example of input for our LSTM\".lower(), train_set.data, char_idx)\n",
    "#print(predict_completions(seq, 5))\n",
    "\n",
    "def plotLineData(header, yLabel, firstData, firstLabel, firstColor='b', xLabel='epoch'):\n",
    "    plt.plot(firstData, color=firstColor)\n",
    "    plt.title(header)\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.xlabel(xLabel)\n",
    "    plt.legend([firstLabel], loc='upper left')\n",
    "    plt.show()\n",
    "plotLineData(\"Loss\", \"loss\", history['loss_train'], \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testdata:\n",
      "\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 18 \n",
      "    0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
      "   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "\n",
      "Columns 19 to 28 \n",
      "    0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   1   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0\n",
      "[torch.DoubleTensor of size 1x30x29]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 870], m2: [29 x 512] at /pytorch/torch/lib/TH/generic/THTensorMath.c:1416",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-ae0bdcfcd9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hrend die historische Zahnrad \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-bc32f5424be9>\u001b[0m in \u001b[0;36mpredict_completion\u001b[0;34m(model, text)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-9913538921fd>\u001b[0m in \u001b[0;36mrnn_predict\u001b[0;34m(model, testdata)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtestdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtestdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-7b632b6d9245>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#x = x.type(torch.DoubleTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (h0, c0 are set to default values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(lstm_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#print(\"LSTM_OUT:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/marvin/anaconda3/lib/python3.5/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(tensor1, tensor2, out)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim_tensor1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim_tensor2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 870], m2: [29 x 512] at /pytorch/torch/lib/TH/generic/THTensorMath.c:1416"
     ]
    }
   ],
   "source": [
    "\n",
    "test = \"hrend die historische Zahnrad \"\n",
    "predict_completion(rnn, test.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "{'ö': 26, 'c': 3, 'd': 4, 'j': 10, 'g': 7, 'p': 16, 'l': 12, 'e': 5, 'r': 17, 'b': 2, 't': 19, 'f': 6, 'ü': 27, ' ': 0, 'u': 20, 'z': 23, 'v': 21, 'k': 11, 'w': 22, 'ß': 24, 'o': 15, 'n': 14, 'ä': 25, 'i': 9, 'h': 8, 's': 18, '–': 28, 'm': 13, 'a': 1}\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "print(train_set.no_classes)\n",
    "print(char_idx)\n",
    "print(args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
