{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding: iso-8859-15 -*-\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration / parameters to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = edict()\n",
    "args.seq_len = 30\n",
    "args.offset = 4\n",
    "args.cuda = False\n",
    "args.batch_size = 1\n",
    "args.num_layers = 3\n",
    "args.hidden_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(textsource):\n",
    "    text = ''\n",
    "    with open(textsource) as txtsource:\n",
    "        for line in txtsource:\n",
    "            line = line.strip().lower()\n",
    "            line = line.replace(',', '').replace('.', '')\n",
    "            line = line.replace('»', '').replace('«', '')\n",
    "            line = line.replace('\"', '')\n",
    "            text += ' ' + line\n",
    "    text = text[:500] #### nachher wieder rauslöschen!!!\n",
    "    return text\n",
    "# Chevrons müssen noch weg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_len, offset):\n",
    "    # Get all the unique characters appearing in the text \n",
    "    chars = sorted(list(set(text)))\n",
    "    char_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "    idx_char = dict((i, c) for i, c in enumerate(chars)) #### das brauchen wir später!!!\n",
    "    no_classes = len(chars) # the nr. of unique characters corresponds to the nr. of classes\n",
    "    \n",
    "    # Define training samples by splitting the text\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - seq_len, offset):\n",
    "        sentences.append(text[i: i + seq_len])\n",
    "        next_chars.append(text[i + seq_len])\n",
    "    print('nr training samples', len(sentences))\n",
    "    \n",
    "    # Generate features and labels using one-hot encoding\n",
    "    X = np.zeros((len(sentences), seq_len, len(chars)), dtype='f')\n",
    "    y = np.zeros((len(sentences)))\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, char in enumerate(sentence):\n",
    "            X[i, j, char_idx[char]] = 1\n",
    "        y[i] = char_idx[next_chars[i]]\n",
    "        \n",
    "    return X, y, no_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    ''' A text dataset class which implements the abstract class torch.utils.data.Dataset. '''\n",
    "    def __init__(self, text, seq_len, offset):\n",
    "        self.data, self.target, self.no_classes = prepare_data(text, seq_len, offset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ''' Get the data for one training sample (by index) '''\n",
    "        return self.data[index,:,:], self.target[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Get the number of training samples '''\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        super(LSTM_RNN, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_shape[0]*input_shape[1], hidden_size = args.hidden_size)\n",
    "        self.linear = nn.Linear(in_features = args.hidden_size, out_features = input_shape[1])\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        # LSTM needs hidden variable which is initialized in self.init_hidden(self)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(args.num_layers, args.batch_size, args.hidden_size))\n",
    "        c0 = Variable(torch.zeros(args.num_layers, args.batch_size, args.hidden_size))\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden) # (h0, c0 are set to default values)\n",
    "        res = self.softmax(self.linear(lstm_out[-1])) # use only the output of the last layer of lstm\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (one epoch)\n",
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss() # use the cross-entropy loss\n",
    "    total_loss = 0.0 # compute total loss over one epoch\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(data.size(0), -1)\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()   \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.long()) # check how far away the output is from the original data\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data[0]\n",
    "    print('Total loss over epoch %s: %s' %(epoch, total_loss/len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nr training samples', 493)\n",
      "(493, 30, 36)\n",
      "36\n",
      " ﻿während die historische zahnradbahn sich mühsam ihren weg den schwindelerregend steilen hang hinaufkrallte blickte edmond kirsch auf die gezackten bergspitzen hoch über ihm in der ferne hineingebaut in die flanke einer senkrecht aufragenden klippe schien das weitläufige klostergebäude über dem abgrund zu schweben als hätte es sich auf magische weise von der felswand gelöst dieser zeitlose zufluchtsort hatte die glühende sonne kataloniens den rauen wind in den bergen und andere unbilden des wetters und der geschichte nun schon seit mehr als vierhundert jahren überdauert ohne je von seiner ursprünglichen bestimmung abzukommen seine bewohner vor der modernen welt abzuschotten und ausgerechnet sie sind die ersten die nun die wahrheit erfahren dachte kirsch was für eine ironie er fragte sich wie sie reagieren würden schließlich waren die gefährlichsten männer auf erden immer und zu allen zeiten männer des glaubens gewesen – insbesondere wenn ihre götter bedroht wurden nicht mehr lange und ich stoße einen flammenden speer in ein hornissennest als die zahnradbahn ihren höchsten punkt erreichte erblickte kirsch eine einsame gestalt die auf dem bahnsteig auf ihn wartete der erschreckend hagere mann trug ein weißes rochett zur violetten soutane eines bischofs dazu ein scheitelkäppchen kirsch kannte die knochigen gesichtszüge seines gastgebers von zahlreichen fotos und verspürte einen unerwarteten adrenalinstoß valdespino nimmt mich persönlich in empfang bischof antonio valdespino war in spanien eine berühmtheit der getreue freund und ratgeber des königshauses galt als einer der prominentesten und einflussreichsten fürsprecher konservativer katholischer werte und einer fortschrittsfeindlichen politischen grundhaltung edmond kirsch nehme ich an sagte der bischof als kirsch aus dem waggon stieg schuldig im sinne der anklage kirsch lächelte und schüttelte valdespinos knochige hand ich danke ihnen dass sie dieses treffen arrangiert haben exzell\n"
     ]
    }
   ],
   "source": [
    "# test the functions defined above\n",
    "textsource = './Brown_Leseprobe.txt'\n",
    "text = prepare_text(textsource)\n",
    "feat, true_pred, no_classes = prepare_data(text, args.seq_len, args.offset)\n",
    "print(feat.shape)\n",
    "print(no_classes)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nr training samples', 118)\n"
     ]
    }
   ],
   "source": [
    "# Generate train and test loader from our data\n",
    "train_text = prepare_text('./Brown_Leseprobe.txt')\n",
    "train_set = TextDataset(train_text, args.seq_len, args.offset)\n",
    "train_loader = DataLoader(train_set, batch_size = args.batch_size, shuffle=True)\n",
    "#dataiter = iter(train_loader)\n",
    "#print(dataiter.next())\n",
    "\n",
    "# set further parameters\n",
    "no_classes = train_set.no_classes\n",
    "input_shape = (args.seq_len, no_classes) # seq_len * nr. of unique characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_RNN(\n",
      "  (lstm): LSTM(900, 128)\n",
      "  (linear): Linear(in_features=128, out_features=30)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Generate model\n",
    "rnn = LSTM_RNN(input_shape)\n",
    "if args.cuda:\n",
    "    rnn.cuda()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimization algorithm\n",
    "optimizer = optim.RMSprop(rnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/immd-user/miniconda/lib/python2.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train(rnn, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.0177 -0.3889 -0.7212  ...   0.4350 -0.6106  0.1054\n",
      "-0.0112 -0.7952  0.1476  ...  -0.2159 -0.3143 -0.3222\n",
      " 0.6312 -1.1751 -0.6134  ...  -0.9334  0.3747 -0.6186\n",
      "          ...             ⋱             ...          \n",
      " 0.0543 -1.7096  0.9420  ...  -1.6969 -0.2117 -0.0515\n",
      "-1.7247 -0.9483  0.1662  ...   1.0048  0.8270  0.0178\n",
      "-0.4684 -3.1775  0.4234  ...   0.5584 -0.3391 -1.6763\n",
      "[torch.FloatTensor of size 128x20]\n",
      "\n",
      "Variable containing:\n",
      " 1.7183e-01  1.7316e-01  3.3180e-01  ...  -1.1094e+00 -3.6481e-01 -3.2444e-02\n",
      "-6.2500e-01  1.0468e+00  2.1427e-01  ...   5.8637e-02 -6.9745e-02 -6.1828e-01\n",
      "-5.1829e-02 -5.7512e-01 -1.4507e-01  ...   8.2479e-01 -2.2266e-01  2.5010e-01\n",
      "                ...                   ⋱                   ...                \n",
      " 7.1955e-01  3.1914e-01  3.4348e-01  ...  -1.0117e+00  6.3292e-02  2.9535e-01\n",
      " 6.2800e-01  3.9284e-02 -1.0128e-01  ...  -6.2670e-01  1.1004e+00 -2.3716e-01\n",
      " 1.1138e+00  1.7880e-01 -1.1428e-01  ...   4.8020e-01 -2.8164e-01 -3.1969e-01\n",
      "[torch.FloatTensor of size 128x30]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "input = autograd.Variable(torch.randn(128, 20))\n",
    "print(input)\n",
    "print(m(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = Variable(torch.randn(5, 3, 10))\n",
    "h0 = Variable(torch.randn(2, 3, 20))\n",
    "c0 = Variable(torch.randn(2, 3, 20))\n",
    "output = rnn(input, (h0, c0))\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 0.8576  0.8065  0.5420  0.6971  0.9650  0.9788  0.8399  0.2232  0.8018  0.6732\n",
      "[torch.FloatTensor of size 1x10]\n",
      ", Variable containing:\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      ")\n",
      "Variable containing:\n",
      " 2.2554\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = Variable(torch.rand(1,10))\n",
    "target = Variable(torch.LongTensor([1]))\n",
    "print(output, target)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
