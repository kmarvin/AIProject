{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- encoding: iso-8859-15 -*-\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import heapq\n",
    "\n",
    "# Import other python files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration / parameters to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_config(config_path = \"config.txt\", args = dict()):\n",
    "    with open(config_path) as source:\n",
    "        for line in source:\n",
    "            line = line.strip()\n",
    "            argLong, valueLong = line.split('=')\n",
    "            arg = argLong.strip()\n",
    "            value = valueLong.strip()\n",
    "            if value == 'True':\n",
    "                value = True\n",
    "            elif value == 'False':\n",
    "                value = False\n",
    "            elif '.' in value:\n",
    "                value = float(value)\n",
    "            else:\n",
    "                value = int(value)\n",
    "            args[arg] = value\n",
    "    return edict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seq_len': 20, 'offset': 1, 'cuda': False, 'batch_size': 64, 'num_layers': 3, 'hidden_size': 256, 'lr': 0.1, 'clip': 1}\n"
     ]
    }
   ],
   "source": [
    "config_path = 'config.txt'\n",
    "args = {}\n",
    "args = set_config(config_path, args)\n",
    "print(args)\n",
    "#args.batch_size = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_text(textsource):\n",
    "    text = ''\n",
    "    with open(textsource, encoding=\"utf8\") as txtsource:\n",
    "        for line in txtsource:\n",
    "            line = line.strip().lower()\n",
    "            line = line.replace(',', '').replace('.', '')\n",
    "            line = line.replace('»', '').replace('«', '')\n",
    "            line = line.replace('\"', '').replace('!', '')\n",
    "            line = line.replace('’', '').replace('*', '')\n",
    "            line = line.replace(u'\\ufeff', '')\n",
    "            text += ' ' + line\n",
    "    text = text[:16020] #### nachher wieder rauslöschen!!!\n",
    "    return text\n",
    "# Chevrons müssen noch weg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(text, seq_len, offset, char_idx, idx_char):\n",
    "      \n",
    "    # Define training samples by splitting the text\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - seq_len, offset):\n",
    "        sentences.append(text[i: i + seq_len])\n",
    "        next_chars.append(text[i + seq_len])\n",
    "\n",
    "    #print('sentences', sentences)    \n",
    "    #print('next_chars', next_chars)\n",
    "    print('nr training samples', len(sentences))\n",
    "    \n",
    "    # Generate features and labels using one-hot encoding\n",
    "    X = np.zeros((len(sentences), seq_len, len(chars)), dtype='f')\n",
    "    y = np.zeros((len(sentences)))\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, char in enumerate(sentence):\n",
    "            X[i, j, char_idx[char]] = 1\n",
    "        y[i] = char_idx[next_chars[i]]\n",
    "        \n",
    "    #print('next_chars: ' + str(next_chars[0]))\n",
    "    #print('out: ' + str(X[0, :, :]))\n",
    "    #print('target: ' + str(y[0]))\n",
    "        \n",
    "    return X, y, char_idx, idx_char, no_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    ''' A text dataset class which implements the abstract class torch.utils.data.Dataset. '''\n",
    "    def __init__(self, text, seq_len, offset, char_idx, idx_char):\n",
    "        self.data, self.target, self.char_idx, self.idx_char, self.no_classes = prepare_data(text, seq_len, \n",
    "                                                                                        offset, char_idx, idx_char)\n",
    "                                                                                                      \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ''' Get the data for one training sample (by index) '''\n",
    "        return self.data[index,:,:], self.target[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Get the number of training samples '''\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM_RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, no_classes):\n",
    "        super(LSTM_RNN, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.GRU(input_size = no_classes, hidden_size = args.hidden_size, num_layers = args.num_layers)\n",
    "        self.linear = nn.Linear(in_features = args.hidden_size, out_features = no_classes)\n",
    "        self.softmax = nn.Softplus()\n",
    "        \n",
    "        nn.init.normal( self.linear.weight, 0, 0.075)\n",
    "        nn.init.normal(self.linear.bias, 0, 0.075)\n",
    "        nn.init.xavier_normal(self.lstm.weight_hh_l0)\n",
    "        nn.init.xavier_normal(self.lstm.weight_ih_l0)\n",
    "        #nn.init.constant(self.lstm.bias, 0.0)\n",
    "\n",
    "        \n",
    "        # LSTM needs hidden variable which is initialized in self.init_hidden(self)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Linear):\n",
    "        #         m.weight.data.normal_(0, 0.075*0.075)\n",
    "        #         m.bias.data.normal_(0, 0.075*0.075)\n",
    "                \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(args.num_layers, args.batch_size, args.hidden_size))\n",
    "        c0 = Variable(torch.zeros(args.num_layers, args.batch_size, args.hidden_size))\n",
    "        return (h0)#,c0)#Variable(torch.zeros((args.num_layers, args.batch_size, args.hidden_size)))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        #x = x.type(torch.DoubleTensor)\n",
    "        #print(x)\n",
    "        lstm_out, hidden = self.lstm(x, hidden) # (h0, c0 are set to default values)\n",
    "        #print(lstm_out)\n",
    "        #print(\"LSTM_OUT:\")\n",
    "        #print(lstm_out)\n",
    "        #lstm_out = lstm_out.view(-1, lstm_out.size(2))\n",
    "        #print(\"----------------\")\n",
    "        #print(lstm_out)\n",
    "        linear_out = self.linear(lstm_out[-1])\n",
    "        #print(\"Linear_OUT:\")\n",
    "        #print(linear_out)\n",
    "        #res = self.softmax(linear_out) # use only the output of the last layer of lstm\n",
    "        return linear_out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training loop (one epoch)\n",
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss() # use the cross-entropy loss\n",
    "    total_loss = 0.0 # compute total loss over one epoch\n",
    "    \n",
    "    #loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #print(data)\n",
    "        data = data.transpose(0, 1) #swap seq_len and batch:size\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        hidden = model.init_hidden()\n",
    "        optimizer.zero_grad()\n",
    "        #print(\"data:\")\n",
    "        #print(data[0, :, :])\n",
    "        \n",
    "        #print(\"output:\")\n",
    "        #print(output[0])\n",
    "        #print(\"target:\")\n",
    "        #print(target)\n",
    "        #print(\"output:\")\n",
    "        #print(output)\n",
    "        for c in range(args.seq_len):\n",
    "            output, hidden = model(data[c, :, :].contiguous().view(1,-1,no_classes), hidden) # // output, hidden = model(data[:, :c, :], hidden)\n",
    "            #loss += criterion(output, data[c+1, :, :].type(torch.LongTensor)) # check how far away the output is from the original data\n",
    "            \n",
    "        #output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target.type(torch.LongTensor)) # check how far away the output is from the original data\n",
    "        #print(\"loss:\")\n",
    "        #print(loss)\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data[0]\n",
    "        #print(total_loss)\n",
    "\n",
    "\n",
    "    relative_loss = total_loss/float(len(train_loader))\n",
    "    print('Mean loss over epoch %s: %s' %(epoch, relative_loss))#loss.data[0]))\n",
    "    return relative_loss # return the relative loss for later analysis\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop (one epoch)\n",
    "def evaluate(model, epoch):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss() # use the cross-entropy loss\n",
    "    total_loss = 0.0 # compute total loss over one epoch\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        #print(data)\n",
    "        data = data.transpose(0, 1)\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        hidden = model.init_hidden()\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, target.type(torch.LongTensor)) # check how far away the output is from the original data\n",
    "\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    relative_loss = total_loss/float(len(train_loader))\n",
    "    print('Mean test loss over epoch %s: %s' %(epoch, relative_loss))#loss.data[0]))\n",
    "    return relative_loss # return the relative loss for later analysis\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction loop for ONE testdata tensor\n",
    "def rnn_predict(model, testdata):\n",
    "    ''' Note: testdata have to be submitted as a tensor'''\n",
    "    testdata = torch.from_numpy(testdata)\n",
    "    #print(\"testdata:\")\n",
    "    #print(testdata)\n",
    "    model.eval()\n",
    "    #testdata = testdata.view(testdata.size(0), -1)\n",
    "    #print(\"testdata:\")\n",
    "    #print(testdata)\n",
    "    if args.cuda:\n",
    "        testdata = testdata.cuda()\n",
    "    testdata = testdata.type(torch.FloatTensor)\n",
    "    testdata = Variable(testdata)\n",
    "    hidden = model.init_hidden()\n",
    "    prediction = model(testdata.unsqueeze(1), hidden)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Function that returns the largest factor of number that isn't the number itself '''\n",
    "def lfactor(num):\n",
    "    for i in range(num - 1, 0, -1): # go backwards from num - 1 to 1\n",
    "        if num % i == 0:            # if a number divides evenly\n",
    "            return i                # it's the largest factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marvins test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# die funktion brauchen wir vllt gar nicht, je nachdem ob wir den test loader verwenden oder wie wir das auch immer machen\n",
    "def prepare_input(text):\n",
    "    X = np.zeros((args.seq_len, no_classes))  # array with one entry which have 20 lines, each 11 entrys\n",
    "    offset = 0\n",
    "    if len(text)<args.seq_len:\n",
    "        offset = args.seq_len-len(text)\n",
    "    for t, char in enumerate(text):          \n",
    "        X[t+offset, char_idx[char]] = 1.\n",
    "    return X\n",
    "\n",
    "def sample(preds, top_n=1):\n",
    "    #print(\"test\")\n",
    "    preds = preds[-1].data.numpy()\n",
    "    #print(preds)\n",
    "    #print(preds.shape)\n",
    "    #preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    return heapq.nlargest(len(preds), zip(preds, itertools.count()))\n",
    "\n",
    "\n",
    "def predict_completion(model, text, topn=1):\n",
    "    original_text = text\n",
    "    processed = text\n",
    " \n",
    "    #print(len(processed))\n",
    "    max_iterations = 10\n",
    "    i = 0\n",
    "    completion = ''\n",
    "    next_char = '' # init\n",
    "    while next_char != ' ' and i < max_iterations:\n",
    "        i += 1\n",
    "        x = prepare_input(text)\n",
    "        preds = rnn_predict(model, x)\n",
    "        next_chars = sample(preds[0], top_n=topn)\n",
    "        #print('id, char: ' + str(next_chars[0][1]) + ', ' + str(idx_char[next_chars[0][1]]))\n",
    "        text = text[1:] + idx_char[next_chars[0][1]]\n",
    "        completion += idx_char[next_chars[0][1]]\n",
    "        next_char = idx_char[next_chars[0][1]]\n",
    "\n",
    "    return completion\n",
    "\n",
    "\n",
    "def predict_completions(model, text, n=3):\n",
    "    x = prepare_input(text)\n",
    "    preds = model.rnn_predict(x, verbose=0)[0]\n",
    "    next_indices = sample(preds, n)\n",
    "    return [idx_char[idx] + predict_completion(text[1:] + idx_char[idx]) for idx in next_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seq_len': 20, 'offset': 1, 'cuda': False, 'batch_size': 64, 'num_layers': 3, 'hidden_size': 256, 'lr': 0.1, 'clip': 1}\nchar_indices_map: {' ': 0, '-': 1, '1': 2, '3': 3, '8': 4, '9': 5, ':': 6, ';': 7, '?': 8, 'a': 9, 'b': 10, 'c': 11, 'd': 12, 'e': 13, 'f': 14, 'g': 15, 'h': 16, 'i': 17, 'j': 18, 'k': 19, 'l': 20, 'm': 21, 'n': 22, 'o': 23, 'p': 24, 'q': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'v': 30, 'w': 31, 'x': 32, 'y': 33, 'z': 34, 'ß': 35, 'ä': 36, 'ö': 37, 'ü': 38, 'ā': 39, '–': 40, '‘': 41, '…': 42, '‹': 43, '›': 44}\nlen(char_indices_map): 45\nnr training samples 16000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr training samples 16000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "config_path = 'config.txt'\n",
    "args = {}\n",
    "args = set_config(config_path, args)\n",
    "print(args)\n",
    "\n",
    "# get whole text to generate char indices map and vice versa\n",
    "text = prepare_text('./brown.txt')\n",
    "# Get all the unique characters appearing in the text \n",
    "chars = sorted(list(set(text)))\n",
    "char_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "print('char_indices_map: ' + str(char_idx))\n",
    "print('len(char_indices_map): ' + str(len(char_idx)))\n",
    "idx_char = dict((i, c) for i, c in enumerate(chars)) #### das brauchen wir später!!!\n",
    "no_classes = len(chars) # the nr. of unique characters corresponds to the nr. of classes\n",
    "\n",
    "# Generate train and test loader from our data\n",
    "train_text = prepare_text('./Brown_Leseprobe.txt')\n",
    "train_set = TextDataset(train_text, args.seq_len, args.offset, char_idx, idx_char)\n",
    "#args.batch_size = lfactor(len(train_set))\n",
    "train_loader = DataLoader(train_set, batch_size = args.batch_size, shuffle=True)\n",
    "\n",
    "# set further parameters\n",
    "char_idx = train_set.char_idx\n",
    "idx_char = train_set.idx_char\n",
    "no_classes = train_set.no_classes\n",
    "input_shape = (args.seq_len, no_classes) # seq_len * nr. of unique characters \n",
    "\n",
    "def dim(a):\n",
    "    if not type(a) == list:\n",
    "        return []\n",
    "    return [len(a)] + dim(a[0])\n",
    "\n",
    "# for i, val in enumerate(train_loader):\n",
    "#     print(val)\n",
    "#     if i==1:\n",
    "#         break\n",
    "\n",
    "test_text = prepare_text('./Brown_Leseprobe_test.txt')\n",
    "test_set = TextDataset(test_text, args.seq_len, args.offset, char_idx, idx_char)\n",
    "test_loader = DataLoader(test_set, batch_size = args.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# get len of data to determine the possible batch_size\n",
    "print(args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 45)\nLSTM_RNN(\n  (lstm): GRU(45, 256, num_layers=3)\n  (linear): Linear(in_features=256, out_features=45)\n  (softmax): Softplus(beta=1, threshold=20)\n)\n"
     ]
    }
   ],
   "source": [
    "# Generate model\n",
    "print(input_shape)\n",
    "rnn = LSTM_RNN(no_classes)\n",
    "if args.cuda:\n",
    "    rnn.cuda()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.load(\"GRU-SoftPlus-epoch#6.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the optimization algorithm\n",
    "optimizer = optim.Adam(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss over epoch 0: 2.5723927140235903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss over epoch 1: 2.101916262626648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss over epoch 2: 1.9320437545776368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss over epoch 3: 1.7953724017143249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss over epoch 4: 1.6566876497268677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss over epoch 5: 1.5084834985733033\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f3d2e5b1817f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m#loss_test = evaluate(rnn, epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-b5527dcb9b8b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epoch)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m#print(\"loss:\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#print(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m#torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython\\miniconda2\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\WinPython\\miniconda2\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# Run training and store history\n",
    "history = dict()\n",
    "history['loss_train'] = []\n",
    "history['loss_test'] = []\n",
    "\n",
    "# wie wir die accuracy machen, weiß ich noch nicht...\n",
    "#history['acc_train'] = []\n",
    "#history['acc_test'] = []\n",
    "print(args.batch_size)\n",
    "for epoch in range(10):\n",
    "    loss_train = train(rnn, epoch)\n",
    "    #loss_test = evaluate(rnn, epoch)\n",
    "    history['loss_train'].append(loss_train)\n",
    "    #history['loss_test'].append(loss_test)\n",
    "    #torch.save(rnn, 'GRU-epoch#{}.pt'.format(epoch))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3d5ccc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#torch.save(rnn, 'GRU-SoftPlus-best.pt')\n",
    "\n",
    "def plotLineData(header, yLabel, firstData, firstLabel, firstColor='b', xLabel='epoch'):\n",
    "    plt.plot(firstData, color=firstColor)\n",
    "    plt.title(header)\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.xlabel(xLabel)\n",
    "    plt.legend([firstLabel], loc='upper right')\n",
    "    plt.savefig(\"loss_train_GRU.png\")\n",
    "    plt.show()\n",
    "plotLineData(\"Loss\", \"loss\", history['loss_train'], \"loss\")\n",
    "# Try a prediction\n",
    "\n",
    "#testdata = Variable(torch.from_numpy(test_set.data[0])) # get first element from the test set\n",
    "#truth = test_set.target[0]\n",
    "#print(testdata,truth)\n",
    "\n",
    "#prediction = rnn(testdata)\n",
    "## dann muss man hier noch auf die sizes achten, ach verdammt\n",
    "#prepare_input(\"This is an example of input for our LSTM\".lower(), train_set.data, char_idx)\n",
    "#print(predict_completions(seq, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahrt \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ten \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elt \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eifen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eichten \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    }
   ],
   "source": [
    "# num_layers = 3 #6\n",
    "testcases = [\"rfolge kleiner Eruptionen in heftige Bew\", \"während die historische zahnradbahn sic\", \n",
    "             \"hrend die historische zahnradbahn sich m\", \"nd die historische zahnradbahn sich mühs\", \n",
    "             \"e historische zahnradbahn sich mühsam ih\", \"torische zahnradbahn sich mühsam ihren w\", \n",
    "             \"che zahnradbahn sich mühsam ihren weg de\", \"hnradbahn sich mühsam ihren weg den schw\", \n",
    "             \"n sich mühsam ihren weg den schwindelerr\", \"am ihren weg den schwindelerregend steil\",\n",
    "             \"hwindelerregend steilen hang hinaufkrall\"]\n",
    "for case in testcases:\n",
    "    print(predict_completion(rnn, case.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebeten \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ien \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recht \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "den \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uklichte \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chten \n"
     ]
    }
   ],
   "source": [
    "# GRU #6\n",
    "testcases = [\"mühsam ihren Weg den schwindel\", \"hen Außenhaut der reptilienart\", \n",
    "             \"chien das weitläufige Klosterg\", \" flanke einer senkrecht aufrag\", \n",
    "             \"irsch auf die gezackten bergsp\", \"ebaut in die flanke einer senk\", \n",
    "             \"in den bergen und andere unbil\", \"hner vor der modernen welt abz\", \n",
    "             \"egend steilen Hang hinaufkrall\", \"uf magische Weise von der fels\",\n",
    "             \"den des Wetters und der Geschi\"]\n",
    "for case in testcases:\n",
    "    print(predict_completion(rnn, case.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebäude \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enden \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itsen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erechittsg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "den \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahinischen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chtte \n"
     ]
    }
   ],
   "source": [
    "# num_layers = 3 #5\n",
    "testcases = [\"mühsam ihren Weg den schwindel\", \"hen Außenhaut der reptilienart\", \n",
    "             \"chien das weitläufige Klosterg\", \" flanke einer senkrecht aufrag\", \n",
    "             \"irsch auf die gezackten bergsp\", \"ebaut in die flanke einer senk\", \n",
    "             \"in den bergen und andere unbil\", \"hner vor der modernen welt abz\", \n",
    "             \"egend steilen Hang hinaufkrall\", \"uf magische Weise von der fels\",\n",
    "             \"den des Wetters und der Geschi\"]\n",
    "for case in testcases:\n",
    "    print(predict_completion(rnn, case.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cher \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dere \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genden \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llte \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itzen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ukommen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uschotten \n"
     ]
    }
   ],
   "source": [
    "# GRU #7\n",
    "testcases = [\"Während die historis\", \"ihren Weg den schwin\", \n",
    "             \"iner senkrecht aufra\", \"re Unbilden des Wett\", \n",
    "             \"eilen Hang hinaufkra\", \"die gezackten Bergsp\", \n",
    "             \"ichen Bestimmung abz\", \"er modernen Welt abz\"]\n",
    "for case in testcases:\n",
    "    print(predict_completion(rnn, case.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tent \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genden \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ert \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itsen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrechen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahinischen\n"
     ]
    }
   ],
   "source": [
    "# num_layers = 3 #2\n",
    "testcases = [\"Während die historis\", \"ihren Weg den schwin\", \n",
    "             \"iner senkrecht aufra\", \"re Unbilden des Wett\", \n",
    "             \"eilen Hang hinaufkra\", \"die gezackten Bergsp\", \n",
    "             \"ichen Bestimmung abz\", \"er modernen Welt abz\"]\n",
    "for case in testcases:\n",
    "    print(predict_completion(rnn, case.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "che \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eiten \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rochen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "und \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uklichte \n"
     ]
    }
   ],
   "source": [
    "# Softplus #5\n",
    "testcases = [\"historis\", \"schw\", \"Garte\", \"senk\", \"Wett\",\n",
    "             \"hinau\", \"die gezackte\", \"Klos\",\n",
    "             \"abz\", \"er modernen Welt abz\"]\n",
    "for case in testcases:\n",
    "    print(predict_completion(rnn, case.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issen \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eit \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ten \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uscholt \n"
     ]
    }
   ],
   "source": [
    "# GRU #5\n",
    "testcases = [\"historis\", \"schw\", \"Garte\", \"senk\", \"Wett\",\n",
    "             \"hinau\", \"die gezackte\", \"Klos\",\n",
    "             \"abz\", \"er modernen Welt abz\"]\n",
    "for case in testcases:\n",
    "    print(predict_completion(rnn, case.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nd \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ud \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahinischen\n"
     ]
    }
   ],
   "source": [
    "# num_layers = 3 #4\n",
    "testcases = [\"historis\", \"schw\", \"Garte\", \"senkrech\", \"Wett\",\n",
    "             \"hinau\", \"die gezackte\", \n",
    "             \"abz\", \"er modernen Welt abz\"]\n",
    "for case in testcases:\n",
    "    print(predict_completion(rnn, case.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n{' ': 0, '!': 1, '*': 2, '-': 3, '1': 4, '3': 5, '7': 6, '8': 7, '9': 8, ':': 9, ';': 10, '?': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37, 'ß': 38, 'ä': 39, 'ó': 40, 'ö': 41, 'ü': 42, 'ā': 43, '–': 44, '‘': 45, '’': 46, '…': 47, '‹': 48, '›': 49}\n8\n"
     ]
    }
   ],
   "source": [
    "print(train_set.no_classes)\n",
    "print(char_idx)\n",
    "print(args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
